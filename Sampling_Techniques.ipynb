{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "knnp = {'n_neighbors': [2,4,5,9,15]}\n",
    "bernoullip = {'alpha': [0.01, 0.1, 0.5,1,10]}\n",
    "logisticp = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "decisionp = {'max_depth': [3, 5, 10,15,20,50]}\n",
    "randomp = {'n_estimators': [100, 200, 500], 'max_depth': [3, 5, 10,15,20,50]}\n",
    "svmp = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "models = [\n",
    "    (KNeighborsClassifier(),knnp),\n",
    "    (BernoulliNB(),bernoullip),\n",
    "    (LogisticRegression(),logisticp),\n",
    "    (DecisionTreeClassifier(),decisionp),\n",
    "    (RandomForestClassifier(), randomp),\n",
    "    (SVC(), svmp)\n",
    "]\n",
    "\n",
    "mnames=['Kneighbors','Bernoulli','logistic','Decision','Random','SVC']\n",
    "table = pd.DataFrame(mnames, columns=['Models'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Creditcard_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      1  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    763\n",
       "1      9\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify=df.Class.value_counts(sort= True)\n",
    "classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "classify.plot(kind = 'bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of observations')\n",
    "plt.title('Imbalanced')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    763\n",
       "1      9\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df['Class']\n",
    "X=df.drop('Class',axis=1)\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    763\n",
       "1    763\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in class over and under sampling were discussed. \n",
    "#In python we have SMOTE to oversample our data. \n",
    "#NearMiss is an under-sampling technique. It aims to balance class distribution by randomly eliminating majority class examples.\n",
    "#Here today I used SMOTE as undersampling will make trhe data size only 18. which is very less to train our data \n",
    "from imblearn.over_sampling import SMOTE\n",
    "smot = SMOTE(random_state = 2)\n",
    "X_new,y_new = smot.fit_resample(X,y)\n",
    "y_new.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    763\n",
       "1    763\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df=pd.concat([X_new,y_new],axis=1)\n",
    "classify_new=new_df.Class.value_counts(sort= True)\n",
    "classify_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "classify_new.plot(kind = 'bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of observations')\n",
    "plt.title('balanced')\n",
    "plt.show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "      Time        V1        V2        V3        V4        V5        V6  \\\n",
      "1361   389 -1.207761 -1.330960  1.696821  0.748215  1.835482  0.463235   \n",
      "511    377  1.166919  0.027049  0.513875  0.860965 -0.519452 -0.681147   \n",
      "9        9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
      "393    284 -0.810756  0.654499  2.217257  0.104341 -0.286801  0.117833   \n",
      "471    346  1.077079  0.284980  0.007731  1.657073  0.052020  0.446389   \n",
      "...    ...       ...       ...       ...       ...       ...       ...   \n",
      "829    240 -0.677673  0.992140 -0.197271  1.337549  0.399417 -0.276864   \n",
      "530    394  1.293053  0.457969 -1.940450  0.173149  2.609570  3.014117   \n",
      "1363     1  1.178929  0.269444  0.169779  0.444303  0.068837 -0.078502   \n",
      "795    277 -0.476912  0.399679  0.675193 -0.119410  0.852615  0.086466   \n",
      "1370   494 -1.078786  0.333963  1.638110  0.026614  0.957878 -0.787730   \n",
      "\n",
      "            V7        V8        V9  ...       V21       V22       V23  \\\n",
      "1361 -1.190303  0.540326  0.545271  ...  0.211753  0.592465  0.241713   \n",
      "511   0.074992 -0.187776  0.345399  ... -0.202750 -0.441391 -0.025782   \n",
      "9     0.651583  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794   \n",
      "393   0.287552 -0.736461  0.699092  ...  0.938194  0.571651 -0.101609   \n",
      "471  -0.407036  0.355704  0.626039  ... -0.174337 -0.174161 -0.153375   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "829  -0.595904  0.533753 -0.956498  ...  0.074639 -0.272713 -0.043013   \n",
      "530  -0.269415  0.754420 -0.221009  ... -0.121126 -0.427753 -0.159336   \n",
      "1363 -0.074467  0.085730 -0.253895  ... -0.224654 -0.635704  0.101863   \n",
      "795   0.288046  0.139842 -0.078547  ... -0.144185 -0.324252  0.025598   \n",
      "1370  0.765609 -0.101326 -0.111330  ...  0.020633  0.193354 -0.258694   \n",
      "\n",
      "           V24       V25       V26       V27       V28     Amount  Class  \n",
      "1361 -1.050852 -0.666459  0.560723  0.011600 -0.017691   1.305054      1  \n",
      "511   0.452607  0.467223  0.262577 -0.023834  0.020521  40.830000      0  \n",
      "9    -0.385050 -0.069733  0.094199  0.246219  0.083076   3.680000      0  \n",
      "393   0.363928 -0.170947 -0.471524  0.058958 -0.079157  30.300000      0  \n",
      "471  -0.466331  0.611001 -0.252871  0.090375  0.054820  10.990000      0  \n",
      "...        ...       ...       ...       ...       ...        ...    ...  \n",
      "829  -0.833690 -0.937696  0.107663  0.241005  0.112587   0.678288      1  \n",
      "530   0.857135  0.850055 -0.311685  0.037536  0.050618   1.000000      0  \n",
      "1363 -0.351685  0.149181  0.125311 -0.006200  0.017215   2.670349      1  \n",
      "795  -1.021802 -1.068921  0.125503  0.056526  0.043753   0.993037      1  \n",
      "1370  0.269573  0.256100 -0.324783 -0.149199 -0.173699   1.000000      1  \n",
      "\n",
      "[384 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "z = 1.96 \n",
    "p=0.5\n",
    "e=0.05\n",
    "srs_size = int((z**2 *p * (1-p))/0.05**2)\n",
    "print(srs_size)\n",
    "srs_sample = new_df.sample(n=srs_size, random_state=0)\n",
    "print(srs_sample)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.620000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.990000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.800000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>264</td>\n",
       "      <td>0.382945</td>\n",
       "      <td>0.369423</td>\n",
       "      <td>0.877133</td>\n",
       "      <td>0.489226</td>\n",
       "      <td>0.162918</td>\n",
       "      <td>-1.002651</td>\n",
       "      <td>0.403228</td>\n",
       "      <td>-0.184363</td>\n",
       "      <td>-0.035655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146214</td>\n",
       "      <td>-0.387687</td>\n",
       "      <td>-0.026119</td>\n",
       "      <td>0.357784</td>\n",
       "      <td>0.285914</td>\n",
       "      <td>-0.124410</td>\n",
       "      <td>-0.055746</td>\n",
       "      <td>-0.031683</td>\n",
       "      <td>2.014954</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>555</td>\n",
       "      <td>0.492592</td>\n",
       "      <td>-0.261778</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>0.974898</td>\n",
       "      <td>-0.052258</td>\n",
       "      <td>-1.067008</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>-0.184951</td>\n",
       "      <td>-0.036315</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118124</td>\n",
       "      <td>-0.597211</td>\n",
       "      <td>0.349717</td>\n",
       "      <td>0.229828</td>\n",
       "      <td>0.231558</td>\n",
       "      <td>0.051743</td>\n",
       "      <td>-0.063206</td>\n",
       "      <td>0.031801</td>\n",
       "      <td>95.161340</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>278</td>\n",
       "      <td>-1.208271</td>\n",
       "      <td>1.420871</td>\n",
       "      <td>-1.050221</td>\n",
       "      <td>2.879562</td>\n",
       "      <td>-0.338765</td>\n",
       "      <td>-1.003062</td>\n",
       "      <td>-1.762814</td>\n",
       "      <td>0.980029</td>\n",
       "      <td>-1.977849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283149</td>\n",
       "      <td>-0.225220</td>\n",
       "      <td>-0.286736</td>\n",
       "      <td>0.112252</td>\n",
       "      <td>0.083160</td>\n",
       "      <td>0.161475</td>\n",
       "      <td>0.176042</td>\n",
       "      <td>-0.093498</td>\n",
       "      <td>0.847480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>517</td>\n",
       "      <td>-1.721457</td>\n",
       "      <td>-1.742412</td>\n",
       "      <td>2.278205</td>\n",
       "      <td>0.890897</td>\n",
       "      <td>2.072778</td>\n",
       "      <td>0.205123</td>\n",
       "      <td>-1.332251</td>\n",
       "      <td>0.542429</td>\n",
       "      <td>0.672296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329778</td>\n",
       "      <td>0.957523</td>\n",
       "      <td>0.153190</td>\n",
       "      <td>-0.533750</td>\n",
       "      <td>-0.060457</td>\n",
       "      <td>0.518827</td>\n",
       "      <td>-0.119372</td>\n",
       "      <td>-0.159272</td>\n",
       "      <td>1.369876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>230</td>\n",
       "      <td>-0.135090</td>\n",
       "      <td>0.519203</td>\n",
       "      <td>0.720383</td>\n",
       "      <td>0.129065</td>\n",
       "      <td>0.852819</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>0.417669</td>\n",
       "      <td>0.077835</td>\n",
       "      <td>-0.127021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088125</td>\n",
       "      <td>-0.243794</td>\n",
       "      <td>0.065839</td>\n",
       "      <td>-0.999654</td>\n",
       "      <td>-1.018342</td>\n",
       "      <td>-0.034811</td>\n",
       "      <td>0.161734</td>\n",
       "      <td>0.156045</td>\n",
       "      <td>0.992083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1137 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0        0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "3        1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "6        4  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
       "7        7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
       "8        7 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "1524   264  0.382945  0.369423  0.877133  0.489226  0.162918 -1.002651   \n",
       "1525   555  0.492592 -0.261778  0.445946  0.974898 -0.052258 -1.067008   \n",
       "1526   278 -1.208271  1.420871 -1.050221  2.879562 -0.338765 -1.003062   \n",
       "1527   517 -1.721457 -1.742412  2.278205  0.890897  2.072778  0.205123   \n",
       "1528   230 -0.135090  0.519203  0.720383  0.129065  0.852819  0.011468   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0     0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
       "3     0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
       "6    -0.005159  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104   \n",
       "7     1.120631 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504   \n",
       "8     0.370145  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1524  0.403228 -0.184363 -0.035655  ... -0.146214 -0.387687 -0.026119   \n",
       "1525  0.135417 -0.184951 -0.036315  ... -0.118124 -0.597211  0.349717   \n",
       "1526 -1.762814  0.980029 -1.977849  ...  0.283149 -0.225220 -0.286736   \n",
       "1527 -1.332251  0.542429  0.672296  ...  0.329778  0.957523  0.153190   \n",
       "1528  0.417669  0.077835 -0.127021  ... -0.088125 -0.243794  0.065839   \n",
       "\n",
       "           V24       V25       V26       V27       V28      Amount  Class  \n",
       "0     0.066928  0.128539 -0.189115  0.133558 -0.021053  149.620000      0  \n",
       "3    -1.175575  0.647376 -0.221929  0.062723  0.061458  123.500000      0  \n",
       "6    -0.780055  0.750137 -0.257237  0.034507  0.005168    4.990000      0  \n",
       "7    -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   40.800000      0  \n",
       "8     1.011592  0.373205 -0.384157  0.011747  0.142404   93.200000      0  \n",
       "...        ...       ...       ...       ...       ...         ...    ...  \n",
       "1524  0.357784  0.285914 -0.124410 -0.055746 -0.031683    2.014954      1  \n",
       "1525  0.229828  0.231558  0.051743 -0.063206  0.031801   95.161340      1  \n",
       "1526  0.112252  0.083160  0.161475  0.176042 -0.093498    0.847480      1  \n",
       "1527 -0.533750 -0.060457  0.518827 -0.119372 -0.159272    1.369876      1  \n",
       "1528 -0.999654 -1.018342 -0.034811  0.161734  0.156045    0.992083      1  \n",
       "\n",
       "[1137 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For this set of code, Chat gpt was used. \n",
    "#A new concept of merging columns with a merge indicator was learnt to find population - sample = test sample\n",
    "merged = pd.merge(new_df, srs_sample, how='left', indicator=True)\n",
    "\n",
    "df_remaining = merged[merged['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "df_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=srs_sample['Class']\n",
    "X_train = srs_sample.drop('Class',axis=1)\n",
    "X_test = df_remaining.drop('Class',axis=1)\n",
    "y_test = df_remaining['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    200\n",
       "1    184\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    579\n",
       "0    558\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 2}\n",
      "{'alpha': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100}\n",
      "{'max_depth': 15}\n",
      "{'max_depth': 10, 'n_estimators': 200}\n",
      "{'C': 100, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "srs_accuracy=[]\n",
    "for model, params in models:\n",
    "    grid = GridSearchCV(model, params, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    print(best_params)\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    accuracy = best_model.score(X_test, y_test)\n",
    "    srs_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>SRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kneighbors</td>\n",
       "      <td>0.803870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.848725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.913808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision</td>\n",
       "      <td>0.940193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random</td>\n",
       "      <td>0.992084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.871592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Models       SRS\n",
       "0  Kneighbors  0.803870\n",
       "1   Bernoulli  0.848725\n",
       "2    logistic  0.913808\n",
       "3    Decision  0.940193\n",
       "4      Random  0.992084\n",
       "5         SVC  0.871592"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['SRS'] = srs_accuracy\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systematic Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "      Time        V1        V2        V3        V4        V5        V6  \\\n",
      "0        0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
      "27      23  1.322707 -0.174041  0.434555  0.576038 -0.836758 -0.831083   \n",
      "54      37  1.295668  0.341483  0.081505  0.566746 -0.110459 -0.766325   \n",
      "81      52  1.147369  0.059035  0.263632  1.211023 -0.044096  0.301067   \n",
      "108     73  1.162281  1.248178 -1.581317  1.475024  1.138357 -1.020373   \n",
      "135     84  1.119272 -0.669639  0.803807 -0.651693 -1.395666 -0.800698   \n",
      "162    103 -0.940893  1.074155  1.759398 -0.601446  0.101693 -0.188520   \n",
      "189    124 -1.710935 -1.366799  2.217311  0.404714 -0.114375 -0.075942   \n",
      "216    142  1.288256  0.085828 -1.179482  0.064357  2.195225  3.383363   \n",
      "243    164 -0.433211  1.020835  2.019730  3.003261  0.031308  0.187063   \n",
      "270    190 -0.549414  0.676861  2.151950  1.014523 -0.620012  0.076154   \n",
      "297    211 -0.247827 -0.282682  1.653354 -1.014865 -0.680433  0.886364   \n",
      "324    237  1.260248 -0.020172 -1.164387  0.266251  2.184886  3.607421   \n",
      "351    259 -1.569485 -1.932133  1.249203 -4.434211  1.244282  0.402688   \n",
      "378    275 -0.363519  0.055464  1.857571 -1.085421 -0.981918 -0.473025   \n",
      "405    292  1.252189 -0.126779  0.280285  0.579416 -0.374125 -0.215217   \n",
      "432    312 -0.719402 -0.124184  1.979309  0.503415 -0.732324  0.394883   \n",
      "459    336 -0.895224  0.562106  2.817524 -0.718734  0.223222  0.796156   \n",
      "486    356  1.586093 -1.169091 -1.350477 -2.504580  1.106389  3.135282   \n",
      "513    379 -1.896099 -1.829046  1.181222  3.897846 -1.686469  1.976371   \n",
      "540    406 -0.814054  1.538222  1.115690 -0.051667  0.092334 -1.013398   \n",
      "567    425 -0.367058  1.035851  1.108064 -0.065459  0.366306 -0.322658   \n",
      "594    446 -1.146103  1.350274  0.907209 -0.040682 -0.242920 -1.099859   \n",
      "621    472 -1.100920  1.029588  1.348333 -1.362082 -0.343465 -0.671659   \n",
      "648    491 -0.946412  0.609500  1.201710  0.113074 -0.210132 -0.954009   \n",
      "675    511 -0.259961  0.998646  1.437975  0.038031  0.125405 -0.957775   \n",
      "702    530  1.217261 -0.080646 -0.059293 -0.868862 -0.236628 -0.700159   \n",
      "729    550 -1.274193  1.722263  0.429337  0.105932 -0.732006 -1.107861   \n",
      "756    564 -0.203837  0.532747 -0.339857 -0.730934  2.728163  3.535882   \n",
      "783    104  0.476731  0.448316  0.348983  0.235098  0.547866  0.131111   \n",
      "810    354 -0.949471 -1.586224  1.709660  0.912247  1.671161  0.371486   \n",
      "837     80  1.234853  0.323521  0.259220  0.615185 -0.234200 -0.755976   \n",
      "864    523 -2.015323 -2.284922  2.274126  1.275356  2.320725  0.498588   \n",
      "891    568  0.890948  0.042776  0.550090  0.741227 -0.040379 -0.880426   \n",
      "918    129  1.114134  0.413500  0.227016  0.823540 -0.377418 -1.084317   \n",
      "945    552 -0.598350  0.171470  0.852125 -0.143946  0.434127 -0.586975   \n",
      "972    546 -1.100494  0.119183  0.999625 -0.369735  0.648374 -0.456979   \n",
      "999    485 -2.805425 -3.006210  1.403224  2.026415  1.611444 -0.686045   \n",
      "1026   548 -0.566256 -1.236402  1.516208  0.942219  1.220722 -0.137244   \n",
      "1053   532 -1.918734 -1.699522  2.067300  0.578835  1.980543  0.317496   \n",
      "1080   540  0.452498  0.377063  0.835265  0.503511  0.130017 -1.006153   \n",
      "1107   489 -2.730074 -2.958396  1.502828  1.943434  1.691074 -0.566184   \n",
      "1134   486 -2.752793 -2.442101  1.110430  1.632425  1.261954 -0.892589   \n",
      "1161   490 -1.086550 -0.029356  1.848403  0.324129  1.187370 -0.680074   \n",
      "1188   234 -0.835159  1.241363 -0.861078  2.501588 -0.276772 -0.859935   \n",
      "1215   407 -2.289073  1.926000 -1.553795  3.934084 -0.497287 -1.417754   \n",
      "1242   532 -1.860852 -0.248030  1.177819 -0.380689  0.961773 -0.364218   \n",
      "1269   537 -1.386105 -1.956090  2.059763  1.055300  1.930239  0.280883   \n",
      "1296   128  0.983771  0.396360  0.336777  0.560425 -0.097344 -0.766909   \n",
      "1323   253  0.449158  0.367970  0.833497  0.504708  0.122339 -1.007784   \n",
      "1350   421 -2.272008  1.378063 -1.083730  3.629111 -0.137089 -1.165768   \n",
      "1377   498 -2.566391 -2.854530  1.719197  1.763176  1.864052 -0.305809   \n",
      "1404   570  0.944486  0.332123  0.398932  0.549792 -0.224146 -0.986390   \n",
      "1431   562  0.306653  0.265706  0.586289  0.262990  0.047994 -0.821266   \n",
      "1458   533 -1.653382  0.089054  1.245302 -0.568421  0.925434 -0.355830   \n",
      "1485   518 -2.028291 -2.099865  2.104485  1.394270  2.196555  0.414504   \n",
      "1512   457 -0.768398  0.394690  1.635892  0.220012  0.868521 -0.913385   \n",
      "\n",
      "            V7        V8        V9  ...       V21       V22       V23  \\\n",
      "0     0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
      "27   -0.264905 -0.220982 -1.071425  ... -0.284376 -0.323357 -0.037710   \n",
      "54    0.073155 -0.168304  0.071837  ... -0.323607 -0.929781  0.063809   \n",
      "81   -0.132960  0.227885  0.252191  ... -0.087813 -0.110756 -0.097771   \n",
      "108   0.638387 -0.136762 -0.805505  ... -0.124012 -0.227150 -0.199185   \n",
      "135  -0.601605  0.014390  2.019905  ...  0.163687  0.546516 -0.176836   \n",
      "162   0.455756 -3.460682  0.441525  ...  2.270069 -0.143518  0.153908   \n",
      "189  -0.259943  0.320897 -0.175355  ...  0.390634  0.481111  0.405839   \n",
      "216  -0.448437  0.799347 -0.147006  ...  0.017485 -0.051355 -0.145480   \n",
      "243   0.850856 -0.143932 -0.918043  ... -0.177298 -0.180260  0.007760   \n",
      "270   0.041578  0.342672  0.124723  ...  0.212024  0.850203 -0.185597   \n",
      "297  -0.538201  0.377970 -1.230879  ...  0.545696  1.400962 -0.133992   \n",
      "324  -0.436997  0.848745  0.172271  ... -0.182469 -0.481653 -0.128006   \n",
      "351  -0.649554  0.534756  0.886183  ... -0.074659  0.397405  0.199030   \n",
      "378   0.210565  0.022670 -1.641689  ... -0.408214 -0.907776  0.248037   \n",
      "405  -0.193078  0.011076  0.770448  ... -0.360296 -0.959573 -0.023837   \n",
      "432   0.280296  0.118478 -1.445666  ... -0.170480 -0.125550  0.203485   \n",
      "459   0.464887 -0.002081  0.387537  ... -0.117009  0.221249 -0.380422   \n",
      "486  -1.351529  0.690932 -2.090697  ... -0.200366 -0.325402 -0.071104   \n",
      "513  -0.564107 -2.400465 -0.979202  ... -0.936346 -0.516626 -0.911682   \n",
      "540   0.748851 -0.124814 -0.207407  ... -0.311452 -0.627544 -0.016469   \n",
      "567   0.596340  0.109672 -0.318671  ... -0.262881 -0.671307 -0.022587   \n",
      "594   0.579042  0.045619  0.460784  ... -0.396947 -0.895759  0.099686   \n",
      "621   0.291222  0.379994  0.338839  ... -0.110388 -0.180427 -0.007197   \n",
      "648   0.130325  0.293270 -0.579076  ... -0.036421 -0.266529 -0.038869   \n",
      "675   0.899758 -0.270159 -0.186834  ... -0.316535 -0.697534 -0.035228   \n",
      "702   0.143747 -0.111374  0.790809  ...  0.081582  0.534693 -0.159644   \n",
      "729  -0.021279  0.818796 -0.447669  ... -0.218749 -0.786584  0.106722   \n",
      "756   0.263680  0.919169 -0.194501  ... -0.082087 -0.271636 -0.157778   \n",
      "783   0.161065  0.119884 -0.170779  ... -0.163739 -0.474502  0.133083   \n",
      "810  -1.441357  0.556805  0.559026  ...  0.209025  0.591533  0.233123   \n",
      "837   0.034104 -0.111234 -0.057288  ... -0.267926 -0.770961  0.119559   \n",
      "864  -2.130408  0.816910  0.782267  ...  0.426939  1.137139  0.261711   \n",
      "891  -0.153874 -0.097887  0.120757  ... -0.207012 -0.593719  0.146819   \n",
      "918  -0.016784 -0.139908 -0.075557  ... -0.255829 -0.801203  0.104668   \n",
      "945   0.202559  0.007086  0.020036  ... -0.220382 -0.431256 -0.113640   \n",
      "972   0.231855  0.065897  0.021545  ... -0.202398 -0.325906 -0.178923   \n",
      "999  -0.230521  0.127667  0.009808  ...  0.607075  0.608970  1.129823   \n",
      "1026 -1.139773  0.348738  0.543101  ...  0.110212  0.307920  0.222925   \n",
      "1053 -1.366996  0.586027  0.666645  ...  0.234412  0.762017  0.123004   \n",
      "1080  0.383361 -0.190514 -0.043224  ... -0.156739 -0.414683 -0.014397   \n",
      "1107 -0.406494  0.189520  0.098653  ...  0.589791  0.663871  1.051933   \n",
      "1134  0.312988 -0.021356 -0.205356  ...  0.474265  0.295666  1.011050   \n",
      "1161  0.437353 -0.016973  0.020548  ...  0.118968  0.416426 -0.175845   \n",
      "1188 -1.501026  0.840908 -1.710089  ...  0.204034 -0.289493 -0.226416   \n",
      "1215 -2.480237  1.365758 -2.726128  ...  0.509690 -0.029756 -0.461744   \n",
      "1242  0.274376  0.121103 -0.004122  ... -0.100724 -0.133240 -0.108420   \n",
      "1269 -1.694456  0.600017  0.780719  ...  0.288687  0.815197  0.265744   \n",
      "1296  0.134871 -0.124268 -0.001155  ... -0.251139 -0.729232  0.133345   \n",
      "1323  0.379198 -0.185766 -0.030278  ... -0.156949 -0.421478 -0.014409   \n",
      "1350 -2.482258  1.313802 -2.288890  ...  0.505001  0.123734 -0.366743   \n",
      "1377 -0.788759  0.323882  0.291651  ...  0.552244  0.783131  0.882733   \n",
      "1404  0.112547 -0.173614  0.015397  ... -0.275638 -0.754942  0.086940   \n",
      "1431  0.149759 -0.098910  0.017315  ... -0.252794 -0.621125  0.004017   \n",
      "1458  0.333200  0.109387  0.006058  ... -0.153696 -0.142270 -0.261468   \n",
      "1485 -2.148184  0.842014  0.627110  ...  0.430883  1.085941  0.229961   \n",
      "1512  0.821063 -0.159973 -0.129150  ...  0.040462  0.199882 -0.229726   \n",
      "\n",
      "           V24       V25       V26       V27       V28      Amount  Class  \n",
      "0     0.066928  0.128539 -0.189115  0.133558 -0.021053  149.620000      0  \n",
      "27    0.347151  0.559639 -0.280158  0.042335  0.028822   16.000000      0  \n",
      "54   -0.193565  0.287574  0.127881 -0.023731  0.025200    0.990000      0  \n",
      "81   -0.323374  0.633279 -0.305328  0.027394 -0.000580    6.670000      0  \n",
      "108  -0.289757  0.776244 -0.283950  0.056747  0.084706    1.000000      0  \n",
      "135   0.402556  0.563402 -0.534236  0.075047  0.042001   67.300000      0  \n",
      "162   0.700927 -0.413235  1.374031 -0.996161 -0.836301    9.990000      0  \n",
      "189   0.066433  0.156732  1.286201 -0.093975  0.098826  230.000000      0  \n",
      "216   1.007613  0.833293 -0.265485  0.020539  0.015394    4.900000      0  \n",
      "243   0.382658 -0.187193  0.100067  0.204039 -0.018150   65.260000      0  \n",
      "270   0.544990 -0.130609 -0.196374  0.422119  0.203313   20.700000      0  \n",
      "297  -0.810782 -0.348544  0.093031  0.165626  0.130422   70.000000      0  \n",
      "324   1.009959  0.892621 -0.326798  0.025039  0.012735    9.990000      0  \n",
      "351  -1.386013 -0.141955 -0.984011  0.274079 -0.019784   55.450000      0  \n",
      "378   0.492936 -0.524889  0.340449  0.011119  0.060936   70.330000      0  \n",
      "405  -0.462201  0.381732  0.340518 -0.034929  0.007525   23.880000      0  \n",
      "432   0.041484 -0.026663 -0.232425  0.134531  0.133484  140.600000      0  \n",
      "459  -0.245721  0.202958  0.320802 -0.174340 -0.331954    7.720000      0  \n",
      "486   0.996898  0.681093 -0.100190  0.022118  0.009259   15.000000      0  \n",
      "513   0.025106  0.656699  0.200868 -0.088994  0.339814  881.130000      0  \n",
      "540   0.363403 -0.014631  0.076914  0.467478  0.228123    1.980000      0  \n",
      "567  -0.347485 -0.197110  0.125083  0.258796  0.090689    3.590000      0  \n",
      "594   0.275643 -0.045217  0.095849  0.563119  0.307945   17.990000      0  \n",
      "621   0.068877 -0.410540  0.731643  0.084051 -0.057236    0.920000      0  \n",
      "648   0.481943 -0.277006  0.949985 -0.079616 -0.016606   18.450000      0  \n",
      "675   0.322387 -0.198797  0.044630  0.077318 -0.125135    8.930000      0  \n",
      "702   0.296812  0.855793 -0.553063  0.054274  0.001410    1.000000      0  \n",
      "729   0.313637 -0.083626  0.083494  0.109637  0.018247   16.830000      0  \n",
      "756   0.989458  0.228821 -0.545156  0.058120  0.035573   12.900000      0  \n",
      "783  -0.994712 -0.827960  0.093614  0.144951  0.152490    1.602949      1  \n",
      "810  -0.686799 -0.092070  0.618981 -0.086548 -0.109891    1.891804      1  \n",
      "837   0.123329  0.200427  0.104626 -0.018782  0.025748    2.690000      1  \n",
      "864  -0.801366 -0.206831  0.828674 -0.106358 -0.169745    1.428984      1  \n",
      "891   0.208023  0.171541  0.180687 -0.033719  0.008205    1.313639      1  \n",
      "918   0.338668  0.209178  0.097985 -0.012126  0.024018    2.583837      1  \n",
      "945  -0.016530 -0.123395  0.184789 -0.222235 -0.225824    1.110358      1  \n",
      "972  -0.113832 -0.216601  0.209245 -0.276356 -0.295290    1.061758      1  \n",
      "999  -0.422408  0.165846  0.084400 -0.223516 -0.011455  408.569136      1  \n",
      "1026 -0.328757 -0.025438  0.523546 -0.079531 -0.082137    1.407557      1  \n",
      "1053 -0.663558 -0.255468  0.667138 -0.193500 -0.237438    1.343820      1  \n",
      "1080  0.358694  0.283745 -0.107574 -0.052492 -0.026786    1.183168      1  \n",
      "1107 -0.463104  0.129787  0.157107 -0.214257 -0.026397  370.459525      1  \n",
      "1134 -0.281253  0.142811 -0.059430 -0.273350 -0.057663  411.360357      1  \n",
      "1161  0.201802  0.300923 -0.259577 -0.107429 -0.132458    1.073877      1  \n",
      "1188  0.041971  0.096220  0.155943  0.147279 -0.076675    1.133909      1  \n",
      "1215  0.321289  0.050317  0.167273  0.255029 -0.142983    0.016728      1  \n",
      "1242 -0.242754 -0.277432  0.204185 -0.336476 -0.344274   50.471589      1  \n",
      "1269 -0.630759 -0.136261  0.716445 -0.105305 -0.132965    1.460397      1  \n",
      "1296 -0.051513 -0.152419  0.090276  0.035194  0.076628    2.299840      1  \n",
      "1323  0.356390  0.280601 -0.107771 -0.053286 -0.026931    2.066214      1  \n",
      "1350  0.168272  0.010472  0.266001  0.211363 -0.146861    0.193569      1  \n",
      "1377 -0.551508  0.051456  0.315047 -0.194145 -0.058856  287.674277      1  \n",
      "1404  0.282431  0.162979  0.109647 -0.055949 -0.012389    1.259683      1  \n",
      "1431  0.158836  0.044587  0.140712 -0.124695 -0.100626    1.197950      1  \n",
      "1458 -0.172000 -0.258705  0.167349 -0.319828 -0.356463    1.000000      1  \n",
      "1485 -0.752379 -0.195852  0.800247 -0.090307 -0.168589    1.366569      1  \n",
      "1512  0.382022  0.378303 -0.413726 -0.098516 -0.114306    1.123626      1  \n",
      "\n",
      "[57 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "n = len(df)\n",
    "m = int(math.sqrt(n))\n",
    "print(m)\n",
    "#evry m row \n",
    "ss_sample = new_df.iloc[::m]\n",
    "print(ss_sample)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(new_df, ss_sample, how='left', indicator=True)\n",
    "df_remaining = merged[merged['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "df_remaining\n",
    "y_train=ss_sample['Class']\n",
    "X_train = ss_sample.drop('Class',axis=1)\n",
    "X_test = df_remaining.drop('Class',axis=1)\n",
    "y_test = df_remaining['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29\n",
       "1    28\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 15}\n",
      "KNeighborsClassifier accuracy: 63.78%\n",
      "{'alpha': 10}\n",
      "BernoulliNB accuracy: 77.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100}\n",
      "LogisticRegression accuracy: 84.34%\n",
      "{'max_depth': 3}\n",
      "DecisionTreeClassifier accuracy: 86.25%\n",
      "{'max_depth': 15, 'n_estimators': 500}\n",
      "RandomForestClassifier accuracy: 91.56%\n",
      "{'C': 1, 'gamma': 0.01}\n",
      "SVC accuracy: 67.73%\n"
     ]
    }
   ],
   "source": [
    "ss_accuracy=[]\n",
    "for model, params in models:\n",
    "    grid = GridSearchCV(model, params, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    print(best_params)\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    accuracy = best_model.score(X_test, y_test)\n",
    "    ss_accuracy.append(accuracy)\n",
    "    print(type(model).__name__ + ' accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>SRS</th>\n",
       "      <th>SS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kneighbors</td>\n",
       "      <td>0.803870</td>\n",
       "      <td>0.637849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.848725</td>\n",
       "      <td>0.773315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.913808</td>\n",
       "      <td>0.843431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision</td>\n",
       "      <td>0.940193</td>\n",
       "      <td>0.862491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.915589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.871592</td>\n",
       "      <td>0.677332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Models       SRS        SS\n",
       "0  Kneighbors  0.803870  0.637849\n",
       "1   Bernoulli  0.848725  0.773315\n",
       "2    logistic  0.913808  0.843431\n",
       "3    Decision  0.940193  0.862491\n",
       "4      Random  0.992084  0.915589\n",
       "5         SVC  0.871592  0.677332"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['SS'] = ss_accuracy\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "strata = 'Class'\n",
    "# Determine the number of strata based on the unique values of the stratification variable\n",
    "S = new_df[strata].nunique()\n",
    "\n",
    "z = 1.96 \n",
    "e=0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 180, 1: 180}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#proportionate stratification:\n",
    "'''\n",
    "    n1 = (N1/N) * n\n",
    "    where:\n",
    "        - n1 is the sample size of stratum 1\n",
    "        - N1 is the population size of stratum 1\n",
    "        - N is the total population size\n",
    "        - n is the sampling size  \n",
    "    as we used smote to balance our dataset therefor n1,n2 will be same. \n",
    "    theerfore we set a minimum sample size say 180\n",
    "       \n",
    "'''\n",
    "\n",
    "sample_size = {}\n",
    "for i in range(S):\n",
    "    Ni = len(new_df[new_df[strata] == i])\n",
    "    pi = new_df[new_df[strata] == i][strata].sum() / Ni\n",
    "    ni = np.ceil((z**2 * pi * (1 - pi)) / ((e / S)**2 ))\n",
    "    sample_size[i] = max(int(ni), 180)\n",
    "\n",
    "\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>265</td>\n",
       "      <td>-0.491003</td>\n",
       "      <td>0.906953</td>\n",
       "      <td>1.645423</td>\n",
       "      <td>-0.083531</td>\n",
       "      <td>-0.195560</td>\n",
       "      <td>-0.710165</td>\n",
       "      <td>0.559119</td>\n",
       "      <td>0.116340</td>\n",
       "      <td>-0.538190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168067</td>\n",
       "      <td>-0.517387</td>\n",
       "      <td>0.018650</td>\n",
       "      <td>0.491652</td>\n",
       "      <td>-0.277795</td>\n",
       "      <td>0.043841</td>\n",
       "      <td>0.253372</td>\n",
       "      <td>0.111749</td>\n",
       "      <td>9.030000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>184</td>\n",
       "      <td>-0.143256</td>\n",
       "      <td>0.743649</td>\n",
       "      <td>1.534072</td>\n",
       "      <td>1.062170</td>\n",
       "      <td>0.208187</td>\n",
       "      <td>-0.838623</td>\n",
       "      <td>0.524151</td>\n",
       "      <td>-0.294661</td>\n",
       "      <td>-0.478856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205014</td>\n",
       "      <td>-0.460893</td>\n",
       "      <td>0.047407</td>\n",
       "      <td>0.339243</td>\n",
       "      <td>-0.779439</td>\n",
       "      <td>0.234456</td>\n",
       "      <td>-0.024125</td>\n",
       "      <td>-0.049898</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>566</td>\n",
       "      <td>1.166360</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>0.497768</td>\n",
       "      <td>0.798920</td>\n",
       "      <td>-0.365524</td>\n",
       "      <td>-0.233421</td>\n",
       "      <td>-0.074210</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.437687</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.195728</td>\n",
       "      <td>-0.365798</td>\n",
       "      <td>0.030729</td>\n",
       "      <td>0.123133</td>\n",
       "      <td>0.381749</td>\n",
       "      <td>0.296735</td>\n",
       "      <td>-0.007175</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>18.560000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>128</td>\n",
       "      <td>1.239495</td>\n",
       "      <td>-0.182609</td>\n",
       "      <td>0.155058</td>\n",
       "      <td>-0.928892</td>\n",
       "      <td>-0.746227</td>\n",
       "      <td>-1.235608</td>\n",
       "      <td>-0.061695</td>\n",
       "      <td>-0.125223</td>\n",
       "      <td>0.984938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146077</td>\n",
       "      <td>0.481119</td>\n",
       "      <td>-0.140019</td>\n",
       "      <td>0.538261</td>\n",
       "      <td>0.710720</td>\n",
       "      <td>-0.621382</td>\n",
       "      <td>0.036867</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>246</td>\n",
       "      <td>-1.069200</td>\n",
       "      <td>1.239963</td>\n",
       "      <td>0.545157</td>\n",
       "      <td>1.005354</td>\n",
       "      <td>-0.025696</td>\n",
       "      <td>-0.910673</td>\n",
       "      <td>0.422442</td>\n",
       "      <td>0.049283</td>\n",
       "      <td>-0.564601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146555</td>\n",
       "      <td>0.602990</td>\n",
       "      <td>0.132656</td>\n",
       "      <td>0.427113</td>\n",
       "      <td>-0.084030</td>\n",
       "      <td>-0.417194</td>\n",
       "      <td>-0.897885</td>\n",
       "      <td>-0.462042</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>486</td>\n",
       "      <td>-2.752793</td>\n",
       "      <td>-2.442101</td>\n",
       "      <td>1.110430</td>\n",
       "      <td>1.632425</td>\n",
       "      <td>1.261954</td>\n",
       "      <td>-0.892589</td>\n",
       "      <td>0.312988</td>\n",
       "      <td>-0.021356</td>\n",
       "      <td>-0.205356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474265</td>\n",
       "      <td>0.295666</td>\n",
       "      <td>1.011050</td>\n",
       "      <td>-0.281253</td>\n",
       "      <td>0.142811</td>\n",
       "      <td>-0.059430</td>\n",
       "      <td>-0.273350</td>\n",
       "      <td>-0.057663</td>\n",
       "      <td>411.360357</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>547</td>\n",
       "      <td>0.147750</td>\n",
       "      <td>-0.544149</td>\n",
       "      <td>0.508604</td>\n",
       "      <td>1.103014</td>\n",
       "      <td>0.085446</td>\n",
       "      <td>-1.066795</td>\n",
       "      <td>0.153961</td>\n",
       "      <td>-0.173526</td>\n",
       "      <td>-0.059196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042076</td>\n",
       "      <td>-0.496504</td>\n",
       "      <td>0.449796</td>\n",
       "      <td>0.178763</td>\n",
       "      <td>0.236262</td>\n",
       "      <td>0.032521</td>\n",
       "      <td>-0.081693</td>\n",
       "      <td>0.032188</td>\n",
       "      <td>137.469116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>511</td>\n",
       "      <td>-0.265778</td>\n",
       "      <td>0.388057</td>\n",
       "      <td>1.306559</td>\n",
       "      <td>0.336589</td>\n",
       "      <td>0.565154</td>\n",
       "      <td>-0.951447</td>\n",
       "      <td>0.641235</td>\n",
       "      <td>-0.172864</td>\n",
       "      <td>-0.094675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040671</td>\n",
       "      <td>-0.052545</td>\n",
       "      <td>-0.141119</td>\n",
       "      <td>0.372580</td>\n",
       "      <td>0.339607</td>\n",
       "      <td>-0.287733</td>\n",
       "      <td>-0.079523</td>\n",
       "      <td>-0.078282</td>\n",
       "      <td>1.087871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>434</td>\n",
       "      <td>-2.188357</td>\n",
       "      <td>1.541878</td>\n",
       "      <td>-1.005903</td>\n",
       "      <td>2.992827</td>\n",
       "      <td>-0.210636</td>\n",
       "      <td>-1.181512</td>\n",
       "      <td>-1.931374</td>\n",
       "      <td>1.121518</td>\n",
       "      <td>-2.166866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366774</td>\n",
       "      <td>-0.068948</td>\n",
       "      <td>-0.421305</td>\n",
       "      <td>0.199777</td>\n",
       "      <td>-0.037441</td>\n",
       "      <td>0.191332</td>\n",
       "      <td>0.130230</td>\n",
       "      <td>-0.195162</td>\n",
       "      <td>0.215934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>488</td>\n",
       "      <td>-1.036590</td>\n",
       "      <td>0.105441</td>\n",
       "      <td>1.814582</td>\n",
       "      <td>0.279531</td>\n",
       "      <td>1.117699</td>\n",
       "      <td>-0.749728</td>\n",
       "      <td>0.576599</td>\n",
       "      <td>-0.060991</td>\n",
       "      <td>-0.030737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102380</td>\n",
       "      <td>0.373848</td>\n",
       "      <td>-0.201736</td>\n",
       "      <td>0.259681</td>\n",
       "      <td>0.329359</td>\n",
       "      <td>-0.320828</td>\n",
       "      <td>-0.106489</td>\n",
       "      <td>-0.130348</td>\n",
       "      <td>1.050585</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "360    265 -0.491003  0.906953  1.645423 -0.083531 -0.195560 -0.710165   \n",
       "262    184 -0.143256  0.743649  1.534072  1.062170  0.208187 -0.838623   \n",
       "757    566  1.166360  0.005061  0.497768  0.798920 -0.365524 -0.233421   \n",
       "195    128  1.239495 -0.182609  0.155058 -0.928892 -0.746227 -1.235608   \n",
       "336    246 -1.069200  1.239963  0.545157  1.005354 -0.025696 -0.910673   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "1134   486 -2.752793 -2.442101  1.110430  1.632425  1.261954 -0.892589   \n",
       "1374   547  0.147750 -0.544149  0.508604  1.103014  0.085446 -1.066795   \n",
       "1248   511 -0.265778  0.388057  1.306559  0.336589  0.565154 -0.951447   \n",
       "1126   434 -2.188357  1.541878 -1.005903  2.992827 -0.210636 -1.181512   \n",
       "899    488 -1.036590  0.105441  1.814582  0.279531  1.117699 -0.749728   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "360   0.559119  0.116340 -0.538190  ... -0.168067 -0.517387  0.018650   \n",
       "262   0.524151 -0.294661 -0.478856  ... -0.205014 -0.460893  0.047407   \n",
       "757  -0.074210 -0.008325  0.437687  ... -0.195728 -0.365798  0.030729   \n",
       "195  -0.061695 -0.125223  0.984938  ...  0.146077  0.481119 -0.140019   \n",
       "336   0.422442  0.049283 -0.564601  ...  0.146555  0.602990  0.132656   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1134  0.312988 -0.021356 -0.205356  ...  0.474265  0.295666  1.011050   \n",
       "1374  0.153961 -0.173526 -0.059196  ... -0.042076 -0.496504  0.449796   \n",
       "1248  0.641235 -0.172864 -0.094675  ... -0.040671 -0.052545 -0.141119   \n",
       "1126 -1.931374  1.121518 -2.166866  ...  0.366774 -0.068948 -0.421305   \n",
       "899   0.576599 -0.060991 -0.030737  ...  0.102380  0.373848 -0.201736   \n",
       "\n",
       "           V24       V25       V26       V27       V28      Amount  Class  \n",
       "360   0.491652 -0.277795  0.043841  0.253372  0.111749    9.030000      0  \n",
       "262   0.339243 -0.779439  0.234456 -0.024125 -0.049898    1.980000      0  \n",
       "757   0.123133  0.381749  0.296735 -0.007175  0.011905   18.560000      0  \n",
       "195   0.538261  0.710720 -0.621382  0.036867  0.010963    8.800000      0  \n",
       "336   0.427113 -0.084030 -0.417194 -0.897885 -0.462042    2.980000      0  \n",
       "...        ...       ...       ...       ...       ...         ...    ...  \n",
       "1134 -0.281253  0.142811 -0.059430 -0.273350 -0.057663  411.360357      1  \n",
       "1374  0.178763  0.236262  0.032521 -0.081693  0.032188  137.469116      1  \n",
       "1248  0.372580  0.339607 -0.287733 -0.079523 -0.078282    1.087871      1  \n",
       "1126  0.199777 -0.037441  0.191332  0.130230 -0.195162    0.215934      1  \n",
       "899   0.259681  0.329359 -0.320828 -0.106489 -0.130348    1.050585      1  \n",
       "\n",
       "[360 rows x 31 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stra_sample = pd.concat([new_df.loc[new_df[strata] == i].sample(n=sample_size[i], random_state=42) for i in range(S)])\n",
    "stra_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(new_df, stra_sample, how='left', indicator=True)\n",
    "df_remaining = merged[merged['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "df_remaining\n",
    "y_train=stra_sample['Class']\n",
    "X_train = stra_sample.drop('Class',axis=1)\n",
    "X_test = df_remaining.drop('Class',axis=1)\n",
    "y_test = df_remaining['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    180\n",
       "1    180\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 2}\n",
      "KNeighborsClassifier accuracy: 79.69%\n",
      "{'alpha': 0.01}\n",
      "BernoulliNB accuracy: 85.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100}\n",
      "LogisticRegression accuracy: 90.28%\n",
      "{'max_depth': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier accuracy: 94.58%\n",
      "{'max_depth': 10, 'n_estimators': 100}\n",
      "RandomForestClassifier accuracy: 98.97%\n",
      "{'C': 100, 'gamma': 0.001}\n",
      "SVC accuracy: 88.98%\n"
     ]
    }
   ],
   "source": [
    "stra_accuracy=[]\n",
    "for model, params in models:\n",
    "    grid = GridSearchCV(model, params, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    print(best_params)\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    accuracy = best_model.score(X_test, y_test)\n",
    "    stra_accuracy.append(accuracy)\n",
    "    print(type(model).__name__ + ' accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>SRS</th>\n",
       "      <th>SS</th>\n",
       "      <th>Stra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kneighbors</td>\n",
       "      <td>0.803870</td>\n",
       "      <td>0.637849</td>\n",
       "      <td>0.796902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.848725</td>\n",
       "      <td>0.773315</td>\n",
       "      <td>0.858864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.913808</td>\n",
       "      <td>0.843431</td>\n",
       "      <td>0.902754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision</td>\n",
       "      <td>0.940193</td>\n",
       "      <td>0.862491</td>\n",
       "      <td>0.945783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.915589</td>\n",
       "      <td>0.989673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.871592</td>\n",
       "      <td>0.677332</td>\n",
       "      <td>0.889845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Models       SRS        SS      Stra\n",
       "0  Kneighbors  0.803870  0.637849  0.796902\n",
       "1   Bernoulli  0.848725  0.773315  0.858864\n",
       "2    logistic  0.913808  0.843431  0.902754\n",
       "3    Decision  0.940193  0.862491  0.945783\n",
       "4      Random  0.992084  0.915589  0.989673\n",
       "5         SVC  0.871592  0.677332  0.889845"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['Stra'] = stra_accuracy\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUSTER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sizes:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1150\n",
       "8     184\n",
       "5      75\n",
       "3      48\n",
       "7      27\n",
       "9      25\n",
       "4       7\n",
       "1       7\n",
       "6       2\n",
       "2       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "data= new_df\n",
    "n_clusters = 10\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "kmeans.fit(X_new[['Amount']])\n",
    "\n",
    "cluster_labels = kmeans.labels_\n",
    "data['clusters'] = cluster_labels\n",
    "# number of samples in each cluster\n",
    "print(\"Cluster sizes:\")\n",
    "cluster_sizes =pd.Series(cluster_labels).value_counts()\n",
    "cluster_sizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusters\n",
       "0    0.593913\n",
       "1    0.000000\n",
       "2    0.000000\n",
       "3    0.500000\n",
       "4    0.000000\n",
       "5    0.213333\n",
       "6    0.000000\n",
       "7    0.629630\n",
       "8    0.038043\n",
       "9    0.640000\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_proportions = data.groupby('clusters')['Class'].mean()\n",
    "fraud_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#avg size of clusters\n",
    "z=1.96\n",
    "e=0.05\n",
    "c=0\n",
    "for i in range(n_clusters):\n",
    "    c=c+cluster_sizes[i]\n",
    "\n",
    "c = int(c/10) \n",
    "c= round(c/len(df),2)\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 15, 1: 0, 2: 0, 3: 16, 4: 0, 5: 11, 6: 0, 7: 15, 8: 3, 9: 15}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sizes = {}\n",
    "for i in range(n_clusters):\n",
    "    p_i = fraud_proportions[i]\n",
    "    n_i = int(np.ceil((z**2 * p_i * (1 - p_i)) / ((e/c)**2)))\n",
    "    sample_sizes[i] = n_i\n",
    "\n",
    "sample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>125</td>\n",
       "      <td>-2.807065</td>\n",
       "      <td>1.183098</td>\n",
       "      <td>2.974779</td>\n",
       "      <td>2.801477</td>\n",
       "      <td>-0.475110</td>\n",
       "      <td>3.217829</td>\n",
       "      <td>-0.401608</td>\n",
       "      <td>0.384395</td>\n",
       "      <td>2.408675</td>\n",
       "      <td>...</td>\n",
       "      <td>1.461535</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>-0.581133</td>\n",
       "      <td>-0.101785</td>\n",
       "      <td>0.529386</td>\n",
       "      <td>0.362411</td>\n",
       "      <td>0.237647</td>\n",
       "      <td>43.710000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>460</td>\n",
       "      <td>-2.264656</td>\n",
       "      <td>-0.707192</td>\n",
       "      <td>1.080243</td>\n",
       "      <td>-4.657545</td>\n",
       "      <td>-0.157183</td>\n",
       "      <td>-0.345406</td>\n",
       "      <td>0.595042</td>\n",
       "      <td>-0.386704</td>\n",
       "      <td>2.211141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222079</td>\n",
       "      <td>-0.189412</td>\n",
       "      <td>-0.332603</td>\n",
       "      <td>0.050197</td>\n",
       "      <td>-1.168870</td>\n",
       "      <td>-0.208793</td>\n",
       "      <td>-0.486754</td>\n",
       "      <td>50.780000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>263</td>\n",
       "      <td>1.143649</td>\n",
       "      <td>0.128210</td>\n",
       "      <td>0.407255</td>\n",
       "      <td>1.160950</td>\n",
       "      <td>-0.504532</td>\n",
       "      <td>-0.904843</td>\n",
       "      <td>0.111993</td>\n",
       "      <td>-0.151667</td>\n",
       "      <td>0.180408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221754</td>\n",
       "      <td>-0.049750</td>\n",
       "      <td>0.345483</td>\n",
       "      <td>0.495658</td>\n",
       "      <td>-0.440443</td>\n",
       "      <td>0.008209</td>\n",
       "      <td>0.031689</td>\n",
       "      <td>49.990000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>561</td>\n",
       "      <td>-0.711797</td>\n",
       "      <td>-0.983416</td>\n",
       "      <td>2.042490</td>\n",
       "      <td>-2.348505</td>\n",
       "      <td>-1.670333</td>\n",
       "      <td>-0.407697</td>\n",
       "      <td>-0.243285</td>\n",
       "      <td>0.098952</td>\n",
       "      <td>-2.324725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659188</td>\n",
       "      <td>0.153339</td>\n",
       "      <td>0.486741</td>\n",
       "      <td>-0.021313</td>\n",
       "      <td>-0.232520</td>\n",
       "      <td>0.060912</td>\n",
       "      <td>0.115507</td>\n",
       "      <td>149.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>99</td>\n",
       "      <td>-0.883996</td>\n",
       "      <td>-0.150765</td>\n",
       "      <td>2.291791</td>\n",
       "      <td>-0.263452</td>\n",
       "      <td>-0.814535</td>\n",
       "      <td>0.955841</td>\n",
       "      <td>0.097631</td>\n",
       "      <td>0.474047</td>\n",
       "      <td>0.139512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051832</td>\n",
       "      <td>0.110298</td>\n",
       "      <td>-0.260629</td>\n",
       "      <td>-0.097549</td>\n",
       "      <td>1.155439</td>\n",
       "      <td>-0.021199</td>\n",
       "      <td>0.062565</td>\n",
       "      <td>142.710000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>108</td>\n",
       "      <td>-0.817164</td>\n",
       "      <td>0.548008</td>\n",
       "      <td>1.987429</td>\n",
       "      <td>2.347620</td>\n",
       "      <td>0.036888</td>\n",
       "      <td>1.509523</td>\n",
       "      <td>0.692961</td>\n",
       "      <td>0.508962</td>\n",
       "      <td>-1.254297</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.324229</td>\n",
       "      <td>0.231980</td>\n",
       "      <td>-0.335248</td>\n",
       "      <td>0.161932</td>\n",
       "      <td>0.014694</td>\n",
       "      <td>-0.028691</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>148.430000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>266</td>\n",
       "      <td>0.982539</td>\n",
       "      <td>-0.229085</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>1.444009</td>\n",
       "      <td>0.200645</td>\n",
       "      <td>0.636756</td>\n",
       "      <td>0.012166</td>\n",
       "      <td>0.128519</td>\n",
       "      <td>-0.361986</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.347024</td>\n",
       "      <td>-0.033817</td>\n",
       "      <td>-1.359832</td>\n",
       "      <td>-0.005459</td>\n",
       "      <td>0.706075</td>\n",
       "      <td>-0.091822</td>\n",
       "      <td>0.024234</td>\n",
       "      <td>154.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>511</td>\n",
       "      <td>-2.320490</td>\n",
       "      <td>-2.698492</td>\n",
       "      <td>2.044250</td>\n",
       "      <td>1.492373</td>\n",
       "      <td>2.123919</td>\n",
       "      <td>0.085353</td>\n",
       "      <td>-1.363037</td>\n",
       "      <td>0.525734</td>\n",
       "      <td>0.581591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962297</td>\n",
       "      <td>0.628542</td>\n",
       "      <td>-0.684317</td>\n",
       "      <td>-0.066222</td>\n",
       "      <td>0.552321</td>\n",
       "      <td>-0.163931</td>\n",
       "      <td>-0.107619</td>\n",
       "      <td>163.305657</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>147</td>\n",
       "      <td>0.912979</td>\n",
       "      <td>-0.653000</td>\n",
       "      <td>0.298165</td>\n",
       "      <td>0.209546</td>\n",
       "      <td>-0.197231</td>\n",
       "      <td>0.971100</td>\n",
       "      <td>-0.449461</td>\n",
       "      <td>0.281896</td>\n",
       "      <td>0.262493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077306</td>\n",
       "      <td>-0.237707</td>\n",
       "      <td>-1.301415</td>\n",
       "      <td>0.179599</td>\n",
       "      <td>0.469956</td>\n",
       "      <td>-0.024739</td>\n",
       "      <td>0.028338</td>\n",
       "      <td>170.430000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>221</td>\n",
       "      <td>-0.342871</td>\n",
       "      <td>-0.199546</td>\n",
       "      <td>1.976353</td>\n",
       "      <td>-0.003495</td>\n",
       "      <td>-1.170366</td>\n",
       "      <td>0.883501</td>\n",
       "      <td>-0.151879</td>\n",
       "      <td>0.160106</td>\n",
       "      <td>0.137973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>-0.098951</td>\n",
       "      <td>-0.943009</td>\n",
       "      <td>-0.618657</td>\n",
       "      <td>0.253306</td>\n",
       "      <td>0.240271</td>\n",
       "      <td>99.820000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>190</td>\n",
       "      <td>-0.913600</td>\n",
       "      <td>0.162262</td>\n",
       "      <td>0.541429</td>\n",
       "      <td>-1.931799</td>\n",
       "      <td>0.235402</td>\n",
       "      <td>-0.209263</td>\n",
       "      <td>0.770523</td>\n",
       "      <td>-0.407195</td>\n",
       "      <td>-1.374754</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546739</td>\n",
       "      <td>-0.320022</td>\n",
       "      <td>-0.928385</td>\n",
       "      <td>-0.080009</td>\n",
       "      <td>0.908687</td>\n",
       "      <td>-0.286881</td>\n",
       "      <td>0.140450</td>\n",
       "      <td>99.950000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>138</td>\n",
       "      <td>-0.986640</td>\n",
       "      <td>0.054620</td>\n",
       "      <td>0.892189</td>\n",
       "      <td>1.238546</td>\n",
       "      <td>1.153949</td>\n",
       "      <td>-0.443157</td>\n",
       "      <td>1.158793</td>\n",
       "      <td>-0.234933</td>\n",
       "      <td>-0.748284</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.590576</td>\n",
       "      <td>0.432953</td>\n",
       "      <td>-0.337631</td>\n",
       "      <td>-0.272391</td>\n",
       "      <td>-0.551475</td>\n",
       "      <td>0.171670</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>148.810000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>489</td>\n",
       "      <td>-0.932349</td>\n",
       "      <td>0.789123</td>\n",
       "      <td>0.534111</td>\n",
       "      <td>1.140724</td>\n",
       "      <td>-0.448967</td>\n",
       "      <td>-0.288526</td>\n",
       "      <td>1.072543</td>\n",
       "      <td>0.118515</td>\n",
       "      <td>-0.152883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662627</td>\n",
       "      <td>0.319256</td>\n",
       "      <td>0.397955</td>\n",
       "      <td>-0.454037</td>\n",
       "      <td>-0.338746</td>\n",
       "      <td>0.134917</td>\n",
       "      <td>-0.094342</td>\n",
       "      <td>139.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>98</td>\n",
       "      <td>1.027114</td>\n",
       "      <td>-1.272543</td>\n",
       "      <td>0.673656</td>\n",
       "      <td>-0.747436</td>\n",
       "      <td>-1.299107</td>\n",
       "      <td>0.293656</td>\n",
       "      <td>-1.054200</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>-0.574991</td>\n",
       "      <td>...</td>\n",
       "      <td>1.101912</td>\n",
       "      <td>-0.248747</td>\n",
       "      <td>-0.259414</td>\n",
       "      <td>0.296058</td>\n",
       "      <td>-0.030996</td>\n",
       "      <td>0.018465</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>494</td>\n",
       "      <td>-1.182051</td>\n",
       "      <td>-0.287028</td>\n",
       "      <td>1.913052</td>\n",
       "      <td>0.409380</td>\n",
       "      <td>1.320550</td>\n",
       "      <td>-0.546925</td>\n",
       "      <td>0.171175</td>\n",
       "      <td>0.067171</td>\n",
       "      <td>0.118582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497816</td>\n",
       "      <td>-0.126352</td>\n",
       "      <td>0.091163</td>\n",
       "      <td>0.246565</td>\n",
       "      <td>-0.142492</td>\n",
       "      <td>-0.109225</td>\n",
       "      <td>-0.136491</td>\n",
       "      <td>1.118400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>69</td>\n",
       "      <td>-1.766645</td>\n",
       "      <td>2.352984</td>\n",
       "      <td>-0.009955</td>\n",
       "      <td>-0.363736</td>\n",
       "      <td>1.460953</td>\n",
       "      <td>-0.204833</td>\n",
       "      <td>0.905819</td>\n",
       "      <td>-3.384123</td>\n",
       "      <td>0.388546</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.883218</td>\n",
       "      <td>-0.247698</td>\n",
       "      <td>-0.758606</td>\n",
       "      <td>0.086450</td>\n",
       "      <td>0.202790</td>\n",
       "      <td>-0.898858</td>\n",
       "      <td>-0.944337</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>344</td>\n",
       "      <td>-0.492952</td>\n",
       "      <td>0.464595</td>\n",
       "      <td>1.181024</td>\n",
       "      <td>0.153257</td>\n",
       "      <td>0.904070</td>\n",
       "      <td>-0.400310</td>\n",
       "      <td>0.625865</td>\n",
       "      <td>-0.027956</td>\n",
       "      <td>-0.133834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006796</td>\n",
       "      <td>-0.080287</td>\n",
       "      <td>-0.374617</td>\n",
       "      <td>-0.382285</td>\n",
       "      <td>-0.223915</td>\n",
       "      <td>0.041612</td>\n",
       "      <td>0.028870</td>\n",
       "      <td>0.995656</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>263</td>\n",
       "      <td>-0.238061</td>\n",
       "      <td>0.503490</td>\n",
       "      <td>0.852928</td>\n",
       "      <td>0.136026</td>\n",
       "      <td>0.867566</td>\n",
       "      <td>-0.107017</td>\n",
       "      <td>0.477575</td>\n",
       "      <td>0.047395</td>\n",
       "      <td>-0.128981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175600</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>-0.819806</td>\n",
       "      <td>-0.835323</td>\n",
       "      <td>-0.089224</td>\n",
       "      <td>0.127170</td>\n",
       "      <td>0.119452</td>\n",
       "      <td>0.993111</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>282</td>\n",
       "      <td>-0.669415</td>\n",
       "      <td>-0.169044</td>\n",
       "      <td>1.301837</td>\n",
       "      <td>0.370525</td>\n",
       "      <td>0.950911</td>\n",
       "      <td>-0.413911</td>\n",
       "      <td>-0.149038</td>\n",
       "      <td>0.127021</td>\n",
       "      <td>0.463116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890770</td>\n",
       "      <td>0.012742</td>\n",
       "      <td>-0.407764</td>\n",
       "      <td>-0.678186</td>\n",
       "      <td>-0.513178</td>\n",
       "      <td>0.253350</td>\n",
       "      <td>0.254336</td>\n",
       "      <td>11.640000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>415</td>\n",
       "      <td>-2.288051</td>\n",
       "      <td>1.606998</td>\n",
       "      <td>-1.293595</td>\n",
       "      <td>3.776220</td>\n",
       "      <td>-0.290702</td>\n",
       "      <td>-1.269790</td>\n",
       "      <td>-2.504249</td>\n",
       "      <td>1.344858</td>\n",
       "      <td>-2.480836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060397</td>\n",
       "      <td>-0.406021</td>\n",
       "      <td>0.228874</td>\n",
       "      <td>0.024053</td>\n",
       "      <td>0.230834</td>\n",
       "      <td>0.231221</td>\n",
       "      <td>-0.145431</td>\n",
       "      <td>0.116356</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>536</td>\n",
       "      <td>-0.736062</td>\n",
       "      <td>0.104864</td>\n",
       "      <td>2.356355</td>\n",
       "      <td>-1.796916</td>\n",
       "      <td>-0.727809</td>\n",
       "      <td>-0.177561</td>\n",
       "      <td>-0.264346</td>\n",
       "      <td>0.179152</td>\n",
       "      <td>-0.672160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432163</td>\n",
       "      <td>-0.347870</td>\n",
       "      <td>-0.126574</td>\n",
       "      <td>0.487953</td>\n",
       "      <td>-0.235765</td>\n",
       "      <td>0.304176</td>\n",
       "      <td>0.126340</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>435</td>\n",
       "      <td>-1.788964</td>\n",
       "      <td>1.364592</td>\n",
       "      <td>-0.343040</td>\n",
       "      <td>2.555589</td>\n",
       "      <td>0.040555</td>\n",
       "      <td>-1.227869</td>\n",
       "      <td>-1.245844</td>\n",
       "      <td>0.806355</td>\n",
       "      <td>-1.776606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084573</td>\n",
       "      <td>-0.386864</td>\n",
       "      <td>0.344841</td>\n",
       "      <td>0.175548</td>\n",
       "      <td>-0.060967</td>\n",
       "      <td>0.122936</td>\n",
       "      <td>-0.136656</td>\n",
       "      <td>0.378042</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>204</td>\n",
       "      <td>-0.188424</td>\n",
       "      <td>0.877602</td>\n",
       "      <td>-0.734686</td>\n",
       "      <td>-0.913404</td>\n",
       "      <td>1.941770</td>\n",
       "      <td>4.037423</td>\n",
       "      <td>-1.707118</td>\n",
       "      <td>-2.537641</td>\n",
       "      <td>-0.590338</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.042331</td>\n",
       "      <td>0.168265</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>0.171167</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.164538</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>241</td>\n",
       "      <td>-1.142321</td>\n",
       "      <td>0.626405</td>\n",
       "      <td>2.526917</td>\n",
       "      <td>2.827973</td>\n",
       "      <td>0.619263</td>\n",
       "      <td>0.897473</td>\n",
       "      <td>0.536278</td>\n",
       "      <td>-0.060163</td>\n",
       "      <td>-0.813749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269173</td>\n",
       "      <td>0.177396</td>\n",
       "      <td>-0.019578</td>\n",
       "      <td>0.048651</td>\n",
       "      <td>0.068831</td>\n",
       "      <td>-0.246503</td>\n",
       "      <td>-0.230837</td>\n",
       "      <td>10.620000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>528</td>\n",
       "      <td>-0.378417</td>\n",
       "      <td>0.751515</td>\n",
       "      <td>1.772256</td>\n",
       "      <td>0.311020</td>\n",
       "      <td>-0.329130</td>\n",
       "      <td>-0.746206</td>\n",
       "      <td>0.719034</td>\n",
       "      <td>-0.081805</td>\n",
       "      <td>-0.152417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240394</td>\n",
       "      <td>-0.057803</td>\n",
       "      <td>0.733812</td>\n",
       "      <td>-0.049448</td>\n",
       "      <td>0.207357</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.057469</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>5</td>\n",
       "      <td>1.156233</td>\n",
       "      <td>0.275225</td>\n",
       "      <td>0.175572</td>\n",
       "      <td>0.437541</td>\n",
       "      <td>0.084320</td>\n",
       "      <td>-0.071727</td>\n",
       "      <td>-0.066854</td>\n",
       "      <td>0.086834</td>\n",
       "      <td>-0.251208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.630494</td>\n",
       "      <td>0.102872</td>\n",
       "      <td>-0.372469</td>\n",
       "      <td>0.117597</td>\n",
       "      <td>0.124286</td>\n",
       "      <td>-0.001315</td>\n",
       "      <td>0.021587</td>\n",
       "      <td>2.635848</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>499</td>\n",
       "      <td>-0.554782</td>\n",
       "      <td>0.392480</td>\n",
       "      <td>1.496188</td>\n",
       "      <td>0.269426</td>\n",
       "      <td>0.740235</td>\n",
       "      <td>-0.929436</td>\n",
       "      <td>0.744993</td>\n",
       "      <td>-0.165763</td>\n",
       "      <td>-0.115377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093165</td>\n",
       "      <td>-0.192106</td>\n",
       "      <td>0.378167</td>\n",
       "      <td>0.362084</td>\n",
       "      <td>-0.360221</td>\n",
       "      <td>-0.090399</td>\n",
       "      <td>-0.099002</td>\n",
       "      <td>1.049528</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>119</td>\n",
       "      <td>1.219421</td>\n",
       "      <td>0.356318</td>\n",
       "      <td>0.306976</td>\n",
       "      <td>0.675745</td>\n",
       "      <td>-0.335587</td>\n",
       "      <td>-1.030548</td>\n",
       "      <td>0.093076</td>\n",
       "      <td>-0.192551</td>\n",
       "      <td>0.030401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.819140</td>\n",
       "      <td>0.128772</td>\n",
       "      <td>0.288253</td>\n",
       "      <td>0.167725</td>\n",
       "      <td>0.094124</td>\n",
       "      <td>-0.015690</td>\n",
       "      <td>0.036879</td>\n",
       "      <td>2.638928</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>230</td>\n",
       "      <td>-0.135090</td>\n",
       "      <td>0.519203</td>\n",
       "      <td>0.720383</td>\n",
       "      <td>0.129065</td>\n",
       "      <td>0.852819</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>0.417669</td>\n",
       "      <td>0.077835</td>\n",
       "      <td>-0.127021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243794</td>\n",
       "      <td>0.065839</td>\n",
       "      <td>-0.999654</td>\n",
       "      <td>-1.018342</td>\n",
       "      <td>-0.034811</td>\n",
       "      <td>0.161734</td>\n",
       "      <td>0.156045</td>\n",
       "      <td>0.992083</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>288</td>\n",
       "      <td>-0.598820</td>\n",
       "      <td>0.073254</td>\n",
       "      <td>-0.113786</td>\n",
       "      <td>-2.315189</td>\n",
       "      <td>1.644382</td>\n",
       "      <td>4.389522</td>\n",
       "      <td>3.174203</td>\n",
       "      <td>-1.032390</td>\n",
       "      <td>0.673033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.496473</td>\n",
       "      <td>-0.276571</td>\n",
       "      <td>1.069602</td>\n",
       "      <td>-0.629078</td>\n",
       "      <td>0.401503</td>\n",
       "      <td>-1.346578</td>\n",
       "      <td>-1.228507</td>\n",
       "      <td>380.950000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>445</td>\n",
       "      <td>-2.755146</td>\n",
       "      <td>-1.142447</td>\n",
       "      <td>0.024379</td>\n",
       "      <td>2.962694</td>\n",
       "      <td>0.617638</td>\n",
       "      <td>-1.207468</td>\n",
       "      <td>-0.803439</td>\n",
       "      <td>0.507743</td>\n",
       "      <td>-1.256491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249924</td>\n",
       "      <td>0.649895</td>\n",
       "      <td>-0.051671</td>\n",
       "      <td>0.187015</td>\n",
       "      <td>-0.017907</td>\n",
       "      <td>-0.050109</td>\n",
       "      <td>-0.034841</td>\n",
       "      <td>320.388014</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>500</td>\n",
       "      <td>-2.482916</td>\n",
       "      <td>-1.778236</td>\n",
       "      <td>1.130820</td>\n",
       "      <td>1.023312</td>\n",
       "      <td>1.171128</td>\n",
       "      <td>-0.732718</td>\n",
       "      <td>0.301305</td>\n",
       "      <td>0.021748</td>\n",
       "      <td>-0.144468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165891</td>\n",
       "      <td>0.672330</td>\n",
       "      <td>-0.269605</td>\n",
       "      <td>0.015657</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>-0.292450</td>\n",
       "      <td>-0.144383</td>\n",
       "      <td>302.165426</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>476</td>\n",
       "      <td>-2.175784</td>\n",
       "      <td>-1.698843</td>\n",
       "      <td>1.356187</td>\n",
       "      <td>1.424776</td>\n",
       "      <td>1.198425</td>\n",
       "      <td>-0.997624</td>\n",
       "      <td>0.552596</td>\n",
       "      <td>-0.104218</td>\n",
       "      <td>-0.218105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372266</td>\n",
       "      <td>0.705728</td>\n",
       "      <td>-0.015201</td>\n",
       "      <td>0.325461</td>\n",
       "      <td>-0.271905</td>\n",
       "      <td>-0.191930</td>\n",
       "      <td>-0.030495</td>\n",
       "      <td>312.414820</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>490</td>\n",
       "      <td>-2.674531</td>\n",
       "      <td>-2.249585</td>\n",
       "      <td>1.116343</td>\n",
       "      <td>1.455786</td>\n",
       "      <td>1.235615</td>\n",
       "      <td>-0.846227</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>-0.008856</td>\n",
       "      <td>-0.187699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258032</td>\n",
       "      <td>0.912823</td>\n",
       "      <td>-0.277875</td>\n",
       "      <td>0.105937</td>\n",
       "      <td>-0.036300</td>\n",
       "      <td>-0.278889</td>\n",
       "      <td>-0.082811</td>\n",
       "      <td>379.694596</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>284</td>\n",
       "      <td>-1.167212</td>\n",
       "      <td>1.263648</td>\n",
       "      <td>-0.109849</td>\n",
       "      <td>-0.783619</td>\n",
       "      <td>-1.472694</td>\n",
       "      <td>-0.212799</td>\n",
       "      <td>1.732683</td>\n",
       "      <td>-0.882945</td>\n",
       "      <td>-0.331912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105265</td>\n",
       "      <td>0.127680</td>\n",
       "      <td>0.509606</td>\n",
       "      <td>-0.401951</td>\n",
       "      <td>0.835236</td>\n",
       "      <td>0.286015</td>\n",
       "      <td>0.067264</td>\n",
       "      <td>302.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>487</td>\n",
       "      <td>-2.747415</td>\n",
       "      <td>-2.428872</td>\n",
       "      <td>1.110836</td>\n",
       "      <td>1.620286</td>\n",
       "      <td>1.260144</td>\n",
       "      <td>-0.889403</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>-0.020497</td>\n",
       "      <td>-0.204143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293080</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>-0.281021</td>\n",
       "      <td>0.140277</td>\n",
       "      <td>-0.057841</td>\n",
       "      <td>-0.273731</td>\n",
       "      <td>-0.059391</td>\n",
       "      <td>409.184350</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>181</td>\n",
       "      <td>-0.723637</td>\n",
       "      <td>0.154496</td>\n",
       "      <td>1.263163</td>\n",
       "      <td>0.964978</td>\n",
       "      <td>-0.027170</td>\n",
       "      <td>2.285202</td>\n",
       "      <td>1.891703</td>\n",
       "      <td>-0.141478</td>\n",
       "      <td>-0.110655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424114</td>\n",
       "      <td>0.206428</td>\n",
       "      <td>-0.966411</td>\n",
       "      <td>-0.761682</td>\n",
       "      <td>-0.437223</td>\n",
       "      <td>-0.051109</td>\n",
       "      <td>-0.380738</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>474</td>\n",
       "      <td>-2.584438</td>\n",
       "      <td>-2.385681</td>\n",
       "      <td>1.230107</td>\n",
       "      <td>1.831599</td>\n",
       "      <td>1.274424</td>\n",
       "      <td>-1.029270</td>\n",
       "      <td>0.445684</td>\n",
       "      <td>-0.087064</td>\n",
       "      <td>-0.242993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402034</td>\n",
       "      <td>1.021365</td>\n",
       "      <td>-0.146404</td>\n",
       "      <td>0.303957</td>\n",
       "      <td>-0.212312</td>\n",
       "      <td>-0.220583</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>414.411734</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>450</td>\n",
       "      <td>-2.809714</td>\n",
       "      <td>-1.523688</td>\n",
       "      <td>0.225719</td>\n",
       "      <td>2.835153</td>\n",
       "      <td>0.758067</td>\n",
       "      <td>-1.180478</td>\n",
       "      <td>-0.589813</td>\n",
       "      <td>0.398843</td>\n",
       "      <td>-1.070013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285034</td>\n",
       "      <td>0.787278</td>\n",
       "      <td>-0.097486</td>\n",
       "      <td>0.204571</td>\n",
       "      <td>-0.042023</td>\n",
       "      <td>-0.088456</td>\n",
       "      <td>-0.021481</td>\n",
       "      <td>359.860499</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.660000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>485</td>\n",
       "      <td>-2.805425</td>\n",
       "      <td>-3.006210</td>\n",
       "      <td>1.403224</td>\n",
       "      <td>2.026415</td>\n",
       "      <td>1.611444</td>\n",
       "      <td>-0.686045</td>\n",
       "      <td>-0.230521</td>\n",
       "      <td>0.127667</td>\n",
       "      <td>0.009808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608970</td>\n",
       "      <td>1.129823</td>\n",
       "      <td>-0.422408</td>\n",
       "      <td>0.165846</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>-0.223516</td>\n",
       "      <td>-0.011455</td>\n",
       "      <td>408.569136</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>550</td>\n",
       "      <td>0.601636</td>\n",
       "      <td>-1.634335</td>\n",
       "      <td>1.042738</td>\n",
       "      <td>1.063768</td>\n",
       "      <td>-1.872933</td>\n",
       "      <td>0.247781</td>\n",
       "      <td>-0.699909</td>\n",
       "      <td>0.211613</td>\n",
       "      <td>-0.248833</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926639</td>\n",
       "      <td>-0.144534</td>\n",
       "      <td>0.484127</td>\n",
       "      <td>0.113315</td>\n",
       "      <td>-0.498560</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.084220</td>\n",
       "      <td>317.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>488</td>\n",
       "      <td>-2.746307</td>\n",
       "      <td>-2.968696</td>\n",
       "      <td>1.481370</td>\n",
       "      <td>1.961311</td>\n",
       "      <td>1.673919</td>\n",
       "      <td>-0.592006</td>\n",
       "      <td>-0.368584</td>\n",
       "      <td>0.176195</td>\n",
       "      <td>0.079513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652043</td>\n",
       "      <td>1.068713</td>\n",
       "      <td>-0.454337</td>\n",
       "      <td>0.137556</td>\n",
       "      <td>0.141443</td>\n",
       "      <td>-0.216252</td>\n",
       "      <td>-0.023178</td>\n",
       "      <td>378.669557</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>93</td>\n",
       "      <td>-0.853807</td>\n",
       "      <td>0.127392</td>\n",
       "      <td>1.267277</td>\n",
       "      <td>0.678584</td>\n",
       "      <td>-1.029851</td>\n",
       "      <td>-0.487614</td>\n",
       "      <td>1.836071</td>\n",
       "      <td>-0.298566</td>\n",
       "      <td>-0.922127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179725</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.724705</td>\n",
       "      <td>0.526798</td>\n",
       "      <td>0.502701</td>\n",
       "      <td>-0.159465</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>322.440000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "190    125 -2.807065  1.183098  2.974779  2.801477 -0.475110  3.217829   \n",
       "608    460 -2.264656 -0.707192  1.080243 -4.657545 -0.157183 -0.345406   \n",
       "357    263  1.143649  0.128210  0.407255  1.160950 -0.504532 -0.904843   \n",
       "751    561 -0.711797 -0.983416  2.042490 -2.348505 -1.670333 -0.407697   \n",
       "158     99 -0.883996 -0.150765  2.291791 -0.263452 -0.814535  0.955841   \n",
       "169    108 -0.817164  0.548008  1.987429  2.347620  0.036888  1.509523   \n",
       "364    266  0.982539 -0.229085  0.003051  1.444009  0.200645  0.636756   \n",
       "1010   511 -2.320490 -2.698492  2.044250  1.492373  2.123919  0.085353   \n",
       "224    147  0.912979 -0.653000  0.298165  0.209546 -0.197231  0.971100   \n",
       "306    221 -0.342871 -0.199546  1.976353 -0.003495 -1.170366  0.883501   \n",
       "267    190 -0.913600  0.162262  0.541429 -1.931799  0.235402 -0.209263   \n",
       "207    138 -0.986640  0.054620  0.892189  1.238546  1.153949 -0.443157   \n",
       "645    489 -0.932349  0.789123  0.534111  1.140724 -0.448967 -0.288526   \n",
       "157     98  1.027114 -1.272543  0.673656 -0.747436 -1.299107  0.293656   \n",
       "804    494 -1.182051 -0.287028  1.913052  0.409380  1.320550 -0.546925   \n",
       "104     69 -1.766645  2.352984 -0.009955 -0.363736  1.460953 -0.204833   \n",
       "1182   344 -0.492952  0.464595  1.181024  0.153257  0.904070 -0.400310   \n",
       "959    263 -0.238061  0.503490  0.852928  0.136026  0.867566 -0.107017   \n",
       "386    282 -0.669415 -0.169044  1.301837  0.370525  0.950911 -0.413911   \n",
       "846    415 -2.288051  1.606998 -1.293595  3.776220 -0.290702 -1.269790   \n",
       "710    536 -0.736062  0.104864  2.356355 -1.796916 -0.727809 -0.177561   \n",
       "879    435 -1.788964  1.364592 -0.343040  2.555589  0.040555 -1.227869   \n",
       "285    204 -0.188424  0.877602 -0.734686 -0.913404  1.941770  4.037423   \n",
       "328    241 -1.142321  0.626405  2.526917  2.827973  0.619263  0.897473   \n",
       "698    528 -0.378417  0.751515  1.772256  0.311020 -0.329130 -0.746206   \n",
       "884      5  1.156233  0.275225  0.175572  0.437541  0.084320 -0.071727   \n",
       "1525   499 -0.554782  0.392480  1.496188  0.269426  0.740235 -0.929436   \n",
       "1312   119  1.219421  0.356318  0.306976  0.675745 -0.335587 -1.030548   \n",
       "1524   230 -0.135090  0.519203  0.720383  0.129065  0.852819  0.011468   \n",
       "396    288 -0.598820  0.073254 -0.113786 -2.315189  1.644382  4.389522   \n",
       "850    445 -2.755146 -1.142447  0.024379  2.962694  0.617638 -1.207468   \n",
       "1506   500 -2.482916 -1.778236  1.130820  1.023312  1.171128 -0.732718   \n",
       "1231   476 -2.175784 -1.698843  1.356187  1.424776  1.198425 -0.997624   \n",
       "1497   490 -2.674531 -2.249585  1.116343  1.455786  1.235615 -0.846227   \n",
       "392    284 -1.167212  1.263648 -0.109849 -0.783619 -1.472694 -0.212799   \n",
       "877    487 -2.747415 -2.428872  1.110836  1.620286  1.260144 -0.889403   \n",
       "257    181 -0.723637  0.154496  1.263163  0.964978 -0.027170  2.285202   \n",
       "1408   474 -2.584438 -2.385681  1.230107  1.831599  1.274424 -1.029270   \n",
       "1138   450 -2.809714 -1.523688  0.225719  2.835153  0.758067 -1.180478   \n",
       "2        1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "999    485 -2.805425 -3.006210  1.403224  2.026415  1.611444 -0.686045   \n",
       "732    550  0.601636 -1.634335  1.042738  1.063768 -1.872933  0.247781   \n",
       "784    488 -2.746307 -2.968696  1.481370  1.961311  1.673919 -0.592006   \n",
       "149     93 -0.853807  0.127392  1.267277  0.678584 -1.029851 -0.487614   \n",
       "\n",
       "            V7        V8        V9  ...       V22       V23       V24  \\\n",
       "190  -0.401608  0.384395  2.408675  ...  1.461535 -0.000330 -0.581133   \n",
       "608   0.595042 -0.386704  2.211141  ...  0.222079 -0.189412 -0.332603   \n",
       "357   0.111993 -0.151667  0.180408  ... -0.221754 -0.049750  0.345483   \n",
       "751  -0.243285  0.098952 -2.324725  ...  0.659188  0.153339  0.486741   \n",
       "158   0.097631  0.474047  0.139512  ...  0.051832  0.110298 -0.260629   \n",
       "169   0.692961  0.508962 -1.254297  ... -0.324229  0.231980 -0.335248   \n",
       "364   0.012166  0.128519 -0.361986  ... -1.347024 -0.033817 -1.359832   \n",
       "1010 -1.363037  0.525734  0.581591  ...  0.962297  0.628542 -0.684317   \n",
       "224  -0.449461  0.281896  0.262493  ... -0.077306 -0.237707 -1.301415   \n",
       "306  -0.151879  0.160106  0.137973  ...  0.086207  0.109600 -0.098951   \n",
       "267   0.770523 -0.407195 -1.374754  ... -0.546739 -0.320022 -0.928385   \n",
       "207   1.158793 -0.234933 -0.748284  ... -0.590576  0.432953 -0.337631   \n",
       "645   1.072543  0.118515 -0.152883  ...  0.662627  0.319256  0.397955   \n",
       "157  -1.054200  0.234900 -0.574991  ...  1.101912 -0.248747 -0.259414   \n",
       "804   0.171175  0.067171  0.118582  ...  0.497816 -0.126352  0.091163   \n",
       "104   0.905819 -3.384123  0.388546  ... -0.883218 -0.247698 -0.758606   \n",
       "1182  0.625865 -0.027956 -0.133834  ... -0.006796 -0.080287 -0.374617   \n",
       "959   0.477575  0.047395 -0.128981  ... -0.175600  0.023793 -0.819806   \n",
       "386  -0.149038  0.127021  0.463116  ...  0.890770  0.012742 -0.407764   \n",
       "846  -2.504249  1.344858 -2.480836  ...  0.060397 -0.406021  0.228874   \n",
       "710  -0.264346  0.179152 -0.672160  ...  0.432163 -0.347870 -0.126574   \n",
       "879  -1.245844  0.806355 -1.776606  ...  0.084573 -0.386864  0.344841   \n",
       "285  -1.707118 -2.537641 -0.590338  ... -1.042331  0.168265  0.963855   \n",
       "328   0.536278 -0.060163 -0.813749  ... -0.269173  0.177396 -0.019578   \n",
       "698   0.719034 -0.081805 -0.152417  ... -0.240394 -0.057803  0.733812   \n",
       "884  -0.066854  0.086834 -0.251208  ... -0.630494  0.102872 -0.372469   \n",
       "1525  0.744993 -0.165763 -0.115377  ...  0.093165 -0.192106  0.378167   \n",
       "1312  0.093076 -0.192551  0.030401  ... -0.819140  0.128772  0.288253   \n",
       "1524  0.417669  0.077835 -0.127021  ... -0.243794  0.065839 -0.999654   \n",
       "396   3.174203 -1.032390  0.673033  ... -0.496473 -0.276571  1.069602   \n",
       "850  -0.803439  0.507743 -1.256491  ...  0.249924  0.649895 -0.051671   \n",
       "1506  0.301305  0.021748 -0.144468  ...  0.165891  0.672330 -0.269605   \n",
       "1231  0.552596 -0.104218 -0.218105  ...  0.372266  0.705728 -0.015201   \n",
       "1497  0.309600 -0.008856 -0.187699  ...  0.258032  0.912823 -0.277875   \n",
       "392   1.732683 -0.882945 -0.331912  ...  0.105265  0.127680  0.509606   \n",
       "877   0.312755 -0.020497 -0.204143  ...  0.293080  1.004300 -0.281021   \n",
       "257   1.891703 -0.141478 -0.110655  ...  0.424114  0.206428 -0.966411   \n",
       "1408  0.445684 -0.087064 -0.242993  ...  0.402034  1.021365 -0.146404   \n",
       "1138 -0.589813  0.398843 -1.070013  ...  0.285034  0.787278 -0.097486   \n",
       "2     0.791461  0.247676 -1.514654  ...  0.771679  0.909412 -0.689281   \n",
       "999  -0.230521  0.127667  0.009808  ...  0.608970  1.129823 -0.422408   \n",
       "732  -0.699909  0.211613 -0.248833  ... -0.926639 -0.144534  0.484127   \n",
       "784  -0.368584  0.176195  0.079513  ...  0.652043  1.068713 -0.454337   \n",
       "149   1.836071 -0.298566 -0.922127  ...  0.179725  0.476744  0.724705   \n",
       "\n",
       "           V25       V26       V27       V28      Amount  Class  clusters  \n",
       "190  -0.101785  0.529386  0.362411  0.237647   43.710000      0         8  \n",
       "608   0.050197 -1.168870 -0.208793 -0.486754   50.780000      0         8  \n",
       "357   0.495658 -0.440443  0.008209  0.031689   49.990000      0         8  \n",
       "751  -0.021313 -0.232520  0.060912  0.115507  149.900000      0         5  \n",
       "158  -0.097549  1.155439 -0.021199  0.062565  142.710000      0         5  \n",
       "169   0.161932  0.014694 -0.028691 -0.015257  148.430000      0         5  \n",
       "364  -0.005459  0.706075 -0.091822  0.024234  154.400000      0         5  \n",
       "1010 -0.066222  0.552321 -0.163931 -0.107619  163.305657      1         5  \n",
       "224   0.179599  0.469956 -0.024739  0.028338  170.430000      0         5  \n",
       "306  -0.943009 -0.618657  0.253306  0.240271   99.820000      0         5  \n",
       "267  -0.080009  0.908687 -0.286881  0.140450   99.950000      0         5  \n",
       "207  -0.272391 -0.551475  0.171670  0.012712  148.810000      0         5  \n",
       "645  -0.454037 -0.338746  0.134917 -0.094342  139.750000      0         5  \n",
       "157   0.296058 -0.030996  0.018465  0.033482  159.000000      0         5  \n",
       "804   0.246565 -0.142492 -0.109225 -0.136491    1.118400      1         0  \n",
       "104   0.086450  0.202790 -0.898858 -0.944337    0.760000      0         0  \n",
       "1182 -0.382285 -0.223915  0.041612  0.028870    0.995656      1         0  \n",
       "959  -0.835323 -0.089224  0.127170  0.119452    0.993111      1         0  \n",
       "386  -0.678186 -0.513178  0.253350  0.254336   11.640000      0         0  \n",
       "846   0.024053  0.230834  0.231221 -0.145431    0.116356      1         0  \n",
       "710   0.487953 -0.235765  0.304176  0.126340   15.000000      0         0  \n",
       "879   0.175548 -0.060967  0.122936 -0.136656    0.378042      1         0  \n",
       "285   0.887850  0.171167  0.012284  0.164538    1.980000      0         0  \n",
       "328   0.048651  0.068831 -0.246503 -0.230837   10.620000      0         0  \n",
       "698  -0.049448  0.207357  0.023386  0.057469   25.410000      0         0  \n",
       "884   0.117597  0.124286 -0.001315  0.021587    2.635848      1         0  \n",
       "1525  0.362084 -0.360221 -0.090399 -0.099002    1.049528      1         0  \n",
       "1312  0.167725  0.094124 -0.015690  0.036879    2.638928      1         0  \n",
       "1524 -1.018342 -0.034811  0.161734  0.156045    0.992083      1         0  \n",
       "396  -0.629078  0.401503 -1.346578 -1.228507  380.950000      0         7  \n",
       "850   0.187015 -0.017907 -0.050109 -0.034841  320.388014      1         7  \n",
       "1506  0.015657  0.020332 -0.292450 -0.144383  302.165426      1         7  \n",
       "1231  0.325461 -0.271905 -0.191930 -0.030495  312.414820      1         7  \n",
       "1497  0.105937 -0.036300 -0.278889 -0.082811  379.694596      1         7  \n",
       "392  -0.401951  0.835236  0.286015  0.067264  302.100000      0         7  \n",
       "877   0.140277 -0.057841 -0.273731 -0.059391  409.184350      1         7  \n",
       "257  -0.761682 -0.437223 -0.051109 -0.380738  318.500000      0         7  \n",
       "1408  0.303957 -0.212312 -0.220583  0.000709  414.411734      1         7  \n",
       "1138  0.204571 -0.042023 -0.088456 -0.021481  359.860499      1         7  \n",
       "2    -0.327642 -0.139097 -0.055353 -0.059752  378.660000      0         7  \n",
       "999   0.165846  0.084400 -0.223516 -0.011455  408.569136      1         7  \n",
       "732   0.113315 -0.498560  0.028261  0.084220  317.250000      0         7  \n",
       "784   0.137556  0.141443 -0.216252 -0.023178  378.669557      1         7  \n",
       "149   0.526798  0.502701 -0.159465  0.002761  322.440000      0         7  \n",
       "\n",
       "[44 rows x 32 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "selected_clusters = np.random.choice(range(n_clusters), size=int(n_clusters * 0.5), replace=False)\n",
    "\n",
    "final_sample = pd.DataFrame()\n",
    "for cluster in selected_clusters:\n",
    "    sample = data[data['clusters'] == cluster].sample(sample_sizes[cluster], replace=False)\n",
    "    final_sample = pd.concat([final_sample, sample])\n",
    "\n",
    "final_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(new_df, final_sample, how='left', indicator=True)\n",
    "df_remaining = merged[merged['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "df_remaining\n",
    "y_train=final_sample['Class']\n",
    "X_train = final_sample.drop('Class', axis=1)\n",
    "X_test = df_remaining.drop('Class',axis=1)\n",
    "y_test = df_remaining['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>125</td>\n",
       "      <td>-2.807065</td>\n",
       "      <td>1.183098</td>\n",
       "      <td>2.974779</td>\n",
       "      <td>2.801477</td>\n",
       "      <td>-0.475110</td>\n",
       "      <td>3.217829</td>\n",
       "      <td>-0.401608</td>\n",
       "      <td>0.384395</td>\n",
       "      <td>2.408675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201782</td>\n",
       "      <td>1.461535</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>-0.581133</td>\n",
       "      <td>-0.101785</td>\n",
       "      <td>0.529386</td>\n",
       "      <td>0.362411</td>\n",
       "      <td>0.237647</td>\n",
       "      <td>43.710000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>460</td>\n",
       "      <td>-2.264656</td>\n",
       "      <td>-0.707192</td>\n",
       "      <td>1.080243</td>\n",
       "      <td>-4.657545</td>\n",
       "      <td>-0.157183</td>\n",
       "      <td>-0.345406</td>\n",
       "      <td>0.595042</td>\n",
       "      <td>-0.386704</td>\n",
       "      <td>2.211141</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.632027</td>\n",
       "      <td>0.222079</td>\n",
       "      <td>-0.189412</td>\n",
       "      <td>-0.332603</td>\n",
       "      <td>0.050197</td>\n",
       "      <td>-1.168870</td>\n",
       "      <td>-0.208793</td>\n",
       "      <td>-0.486754</td>\n",
       "      <td>50.780000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>263</td>\n",
       "      <td>1.143649</td>\n",
       "      <td>0.128210</td>\n",
       "      <td>0.407255</td>\n",
       "      <td>1.160950</td>\n",
       "      <td>-0.504532</td>\n",
       "      <td>-0.904843</td>\n",
       "      <td>0.111993</td>\n",
       "      <td>-0.151667</td>\n",
       "      <td>0.180408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036001</td>\n",
       "      <td>-0.221754</td>\n",
       "      <td>-0.049750</td>\n",
       "      <td>0.345483</td>\n",
       "      <td>0.495658</td>\n",
       "      <td>-0.440443</td>\n",
       "      <td>0.008209</td>\n",
       "      <td>0.031689</td>\n",
       "      <td>49.990000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>561</td>\n",
       "      <td>-0.711797</td>\n",
       "      <td>-0.983416</td>\n",
       "      <td>2.042490</td>\n",
       "      <td>-2.348505</td>\n",
       "      <td>-1.670333</td>\n",
       "      <td>-0.407697</td>\n",
       "      <td>-0.243285</td>\n",
       "      <td>0.098952</td>\n",
       "      <td>-2.324725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230367</td>\n",
       "      <td>0.659188</td>\n",
       "      <td>0.153339</td>\n",
       "      <td>0.486741</td>\n",
       "      <td>-0.021313</td>\n",
       "      <td>-0.232520</td>\n",
       "      <td>0.060912</td>\n",
       "      <td>0.115507</td>\n",
       "      <td>149.900000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>99</td>\n",
       "      <td>-0.883996</td>\n",
       "      <td>-0.150765</td>\n",
       "      <td>2.291791</td>\n",
       "      <td>-0.263452</td>\n",
       "      <td>-0.814535</td>\n",
       "      <td>0.955841</td>\n",
       "      <td>0.097631</td>\n",
       "      <td>0.474047</td>\n",
       "      <td>0.139512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070901</td>\n",
       "      <td>0.051832</td>\n",
       "      <td>0.110298</td>\n",
       "      <td>-0.260629</td>\n",
       "      <td>-0.097549</td>\n",
       "      <td>1.155439</td>\n",
       "      <td>-0.021199</td>\n",
       "      <td>0.062565</td>\n",
       "      <td>142.710000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>108</td>\n",
       "      <td>-0.817164</td>\n",
       "      <td>0.548008</td>\n",
       "      <td>1.987429</td>\n",
       "      <td>2.347620</td>\n",
       "      <td>0.036888</td>\n",
       "      <td>1.509523</td>\n",
       "      <td>0.692961</td>\n",
       "      <td>0.508962</td>\n",
       "      <td>-1.254297</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047336</td>\n",
       "      <td>-0.324229</td>\n",
       "      <td>0.231980</td>\n",
       "      <td>-0.335248</td>\n",
       "      <td>0.161932</td>\n",
       "      <td>0.014694</td>\n",
       "      <td>-0.028691</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>148.430000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>266</td>\n",
       "      <td>0.982539</td>\n",
       "      <td>-0.229085</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>1.444009</td>\n",
       "      <td>0.200645</td>\n",
       "      <td>0.636756</td>\n",
       "      <td>0.012166</td>\n",
       "      <td>0.128519</td>\n",
       "      <td>-0.361986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315217</td>\n",
       "      <td>-1.347024</td>\n",
       "      <td>-0.033817</td>\n",
       "      <td>-1.359832</td>\n",
       "      <td>-0.005459</td>\n",
       "      <td>0.706075</td>\n",
       "      <td>-0.091822</td>\n",
       "      <td>0.024234</td>\n",
       "      <td>154.400000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>511</td>\n",
       "      <td>-2.320490</td>\n",
       "      <td>-2.698492</td>\n",
       "      <td>2.044250</td>\n",
       "      <td>1.492373</td>\n",
       "      <td>2.123919</td>\n",
       "      <td>0.085353</td>\n",
       "      <td>-1.363037</td>\n",
       "      <td>0.525734</td>\n",
       "      <td>0.581591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495838</td>\n",
       "      <td>0.962297</td>\n",
       "      <td>0.628542</td>\n",
       "      <td>-0.684317</td>\n",
       "      <td>-0.066222</td>\n",
       "      <td>0.552321</td>\n",
       "      <td>-0.163931</td>\n",
       "      <td>-0.107619</td>\n",
       "      <td>163.305657</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>147</td>\n",
       "      <td>0.912979</td>\n",
       "      <td>-0.653000</td>\n",
       "      <td>0.298165</td>\n",
       "      <td>0.209546</td>\n",
       "      <td>-0.197231</td>\n",
       "      <td>0.971100</td>\n",
       "      <td>-0.449461</td>\n",
       "      <td>0.281896</td>\n",
       "      <td>0.262493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089258</td>\n",
       "      <td>-0.077306</td>\n",
       "      <td>-0.237707</td>\n",
       "      <td>-1.301415</td>\n",
       "      <td>0.179599</td>\n",
       "      <td>0.469956</td>\n",
       "      <td>-0.024739</td>\n",
       "      <td>0.028338</td>\n",
       "      <td>170.430000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>221</td>\n",
       "      <td>-0.342871</td>\n",
       "      <td>-0.199546</td>\n",
       "      <td>1.976353</td>\n",
       "      <td>-0.003495</td>\n",
       "      <td>-1.170366</td>\n",
       "      <td>0.883501</td>\n",
       "      <td>-0.151879</td>\n",
       "      <td>0.160106</td>\n",
       "      <td>0.137973</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.313443</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>-0.098951</td>\n",
       "      <td>-0.943009</td>\n",
       "      <td>-0.618657</td>\n",
       "      <td>0.253306</td>\n",
       "      <td>0.240271</td>\n",
       "      <td>99.820000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>190</td>\n",
       "      <td>-0.913600</td>\n",
       "      <td>0.162262</td>\n",
       "      <td>0.541429</td>\n",
       "      <td>-1.931799</td>\n",
       "      <td>0.235402</td>\n",
       "      <td>-0.209263</td>\n",
       "      <td>0.770523</td>\n",
       "      <td>-0.407195</td>\n",
       "      <td>-1.374754</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.382552</td>\n",
       "      <td>-0.546739</td>\n",
       "      <td>-0.320022</td>\n",
       "      <td>-0.928385</td>\n",
       "      <td>-0.080009</td>\n",
       "      <td>0.908687</td>\n",
       "      <td>-0.286881</td>\n",
       "      <td>0.140450</td>\n",
       "      <td>99.950000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>138</td>\n",
       "      <td>-0.986640</td>\n",
       "      <td>0.054620</td>\n",
       "      <td>0.892189</td>\n",
       "      <td>1.238546</td>\n",
       "      <td>1.153949</td>\n",
       "      <td>-0.443157</td>\n",
       "      <td>1.158793</td>\n",
       "      <td>-0.234933</td>\n",
       "      <td>-0.748284</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156084</td>\n",
       "      <td>-0.590576</td>\n",
       "      <td>0.432953</td>\n",
       "      <td>-0.337631</td>\n",
       "      <td>-0.272391</td>\n",
       "      <td>-0.551475</td>\n",
       "      <td>0.171670</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>148.810000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>489</td>\n",
       "      <td>-0.932349</td>\n",
       "      <td>0.789123</td>\n",
       "      <td>0.534111</td>\n",
       "      <td>1.140724</td>\n",
       "      <td>-0.448967</td>\n",
       "      <td>-0.288526</td>\n",
       "      <td>1.072543</td>\n",
       "      <td>0.118515</td>\n",
       "      <td>-0.152883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092532</td>\n",
       "      <td>0.662627</td>\n",
       "      <td>0.319256</td>\n",
       "      <td>0.397955</td>\n",
       "      <td>-0.454037</td>\n",
       "      <td>-0.338746</td>\n",
       "      <td>0.134917</td>\n",
       "      <td>-0.094342</td>\n",
       "      <td>139.750000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>98</td>\n",
       "      <td>1.027114</td>\n",
       "      <td>-1.272543</td>\n",
       "      <td>0.673656</td>\n",
       "      <td>-0.747436</td>\n",
       "      <td>-1.299107</td>\n",
       "      <td>0.293656</td>\n",
       "      <td>-1.054200</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>-0.574991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522113</td>\n",
       "      <td>1.101912</td>\n",
       "      <td>-0.248747</td>\n",
       "      <td>-0.259414</td>\n",
       "      <td>0.296058</td>\n",
       "      <td>-0.030996</td>\n",
       "      <td>0.018465</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>494</td>\n",
       "      <td>-1.182051</td>\n",
       "      <td>-0.287028</td>\n",
       "      <td>1.913052</td>\n",
       "      <td>0.409380</td>\n",
       "      <td>1.320550</td>\n",
       "      <td>-0.546925</td>\n",
       "      <td>0.171175</td>\n",
       "      <td>0.067171</td>\n",
       "      <td>0.118582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150677</td>\n",
       "      <td>0.497816</td>\n",
       "      <td>-0.126352</td>\n",
       "      <td>0.091163</td>\n",
       "      <td>0.246565</td>\n",
       "      <td>-0.142492</td>\n",
       "      <td>-0.109225</td>\n",
       "      <td>-0.136491</td>\n",
       "      <td>1.118400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>69</td>\n",
       "      <td>-1.766645</td>\n",
       "      <td>2.352984</td>\n",
       "      <td>-0.009955</td>\n",
       "      <td>-0.363736</td>\n",
       "      <td>1.460953</td>\n",
       "      <td>-0.204833</td>\n",
       "      <td>0.905819</td>\n",
       "      <td>-3.384123</td>\n",
       "      <td>0.388546</td>\n",
       "      <td>...</td>\n",
       "      <td>1.964253</td>\n",
       "      <td>-0.883218</td>\n",
       "      <td>-0.247698</td>\n",
       "      <td>-0.758606</td>\n",
       "      <td>0.086450</td>\n",
       "      <td>0.202790</td>\n",
       "      <td>-0.898858</td>\n",
       "      <td>-0.944337</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>344</td>\n",
       "      <td>-0.492952</td>\n",
       "      <td>0.464595</td>\n",
       "      <td>1.181024</td>\n",
       "      <td>0.153257</td>\n",
       "      <td>0.904070</td>\n",
       "      <td>-0.400310</td>\n",
       "      <td>0.625865</td>\n",
       "      <td>-0.027956</td>\n",
       "      <td>-0.133834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018412</td>\n",
       "      <td>-0.006796</td>\n",
       "      <td>-0.080287</td>\n",
       "      <td>-0.374617</td>\n",
       "      <td>-0.382285</td>\n",
       "      <td>-0.223915</td>\n",
       "      <td>0.041612</td>\n",
       "      <td>0.028870</td>\n",
       "      <td>0.995656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>263</td>\n",
       "      <td>-0.238061</td>\n",
       "      <td>0.503490</td>\n",
       "      <td>0.852928</td>\n",
       "      <td>0.136026</td>\n",
       "      <td>0.867566</td>\n",
       "      <td>-0.107017</td>\n",
       "      <td>0.477575</td>\n",
       "      <td>0.047395</td>\n",
       "      <td>-0.128981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068066</td>\n",
       "      <td>-0.175600</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>-0.819806</td>\n",
       "      <td>-0.835323</td>\n",
       "      <td>-0.089224</td>\n",
       "      <td>0.127170</td>\n",
       "      <td>0.119452</td>\n",
       "      <td>0.993111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>282</td>\n",
       "      <td>-0.669415</td>\n",
       "      <td>-0.169044</td>\n",
       "      <td>1.301837</td>\n",
       "      <td>0.370525</td>\n",
       "      <td>0.950911</td>\n",
       "      <td>-0.413911</td>\n",
       "      <td>-0.149038</td>\n",
       "      <td>0.127021</td>\n",
       "      <td>0.463116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297490</td>\n",
       "      <td>0.890770</td>\n",
       "      <td>0.012742</td>\n",
       "      <td>-0.407764</td>\n",
       "      <td>-0.678186</td>\n",
       "      <td>-0.513178</td>\n",
       "      <td>0.253350</td>\n",
       "      <td>0.254336</td>\n",
       "      <td>11.640000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>415</td>\n",
       "      <td>-2.288051</td>\n",
       "      <td>1.606998</td>\n",
       "      <td>-1.293595</td>\n",
       "      <td>3.776220</td>\n",
       "      <td>-0.290702</td>\n",
       "      <td>-1.269790</td>\n",
       "      <td>-2.504249</td>\n",
       "      <td>1.344858</td>\n",
       "      <td>-2.480836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509880</td>\n",
       "      <td>0.060397</td>\n",
       "      <td>-0.406021</td>\n",
       "      <td>0.228874</td>\n",
       "      <td>0.024053</td>\n",
       "      <td>0.230834</td>\n",
       "      <td>0.231221</td>\n",
       "      <td>-0.145431</td>\n",
       "      <td>0.116356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>536</td>\n",
       "      <td>-0.736062</td>\n",
       "      <td>0.104864</td>\n",
       "      <td>2.356355</td>\n",
       "      <td>-1.796916</td>\n",
       "      <td>-0.727809</td>\n",
       "      <td>-0.177561</td>\n",
       "      <td>-0.264346</td>\n",
       "      <td>0.179152</td>\n",
       "      <td>-0.672160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132683</td>\n",
       "      <td>0.432163</td>\n",
       "      <td>-0.347870</td>\n",
       "      <td>-0.126574</td>\n",
       "      <td>0.487953</td>\n",
       "      <td>-0.235765</td>\n",
       "      <td>0.304176</td>\n",
       "      <td>0.126340</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>435</td>\n",
       "      <td>-1.788964</td>\n",
       "      <td>1.364592</td>\n",
       "      <td>-0.343040</td>\n",
       "      <td>2.555589</td>\n",
       "      <td>0.040555</td>\n",
       "      <td>-1.227869</td>\n",
       "      <td>-1.245844</td>\n",
       "      <td>0.806355</td>\n",
       "      <td>-1.776606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346781</td>\n",
       "      <td>0.084573</td>\n",
       "      <td>-0.386864</td>\n",
       "      <td>0.344841</td>\n",
       "      <td>0.175548</td>\n",
       "      <td>-0.060967</td>\n",
       "      <td>0.122936</td>\n",
       "      <td>-0.136656</td>\n",
       "      <td>0.378042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>204</td>\n",
       "      <td>-0.188424</td>\n",
       "      <td>0.877602</td>\n",
       "      <td>-0.734686</td>\n",
       "      <td>-0.913404</td>\n",
       "      <td>1.941770</td>\n",
       "      <td>4.037423</td>\n",
       "      <td>-1.707118</td>\n",
       "      <td>-2.537641</td>\n",
       "      <td>-0.590338</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.824079</td>\n",
       "      <td>-1.042331</td>\n",
       "      <td>0.168265</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>0.171167</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.164538</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>241</td>\n",
       "      <td>-1.142321</td>\n",
       "      <td>0.626405</td>\n",
       "      <td>2.526917</td>\n",
       "      <td>2.827973</td>\n",
       "      <td>0.619263</td>\n",
       "      <td>0.897473</td>\n",
       "      <td>0.536278</td>\n",
       "      <td>-0.060163</td>\n",
       "      <td>-0.813749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309746</td>\n",
       "      <td>-0.269173</td>\n",
       "      <td>0.177396</td>\n",
       "      <td>-0.019578</td>\n",
       "      <td>0.048651</td>\n",
       "      <td>0.068831</td>\n",
       "      <td>-0.246503</td>\n",
       "      <td>-0.230837</td>\n",
       "      <td>10.620000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>528</td>\n",
       "      <td>-0.378417</td>\n",
       "      <td>0.751515</td>\n",
       "      <td>1.772256</td>\n",
       "      <td>0.311020</td>\n",
       "      <td>-0.329130</td>\n",
       "      <td>-0.746206</td>\n",
       "      <td>0.719034</td>\n",
       "      <td>-0.081805</td>\n",
       "      <td>-0.152417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120891</td>\n",
       "      <td>-0.240394</td>\n",
       "      <td>-0.057803</td>\n",
       "      <td>0.733812</td>\n",
       "      <td>-0.049448</td>\n",
       "      <td>0.207357</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.057469</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>5</td>\n",
       "      <td>1.156233</td>\n",
       "      <td>0.275225</td>\n",
       "      <td>0.175572</td>\n",
       "      <td>0.437541</td>\n",
       "      <td>0.084320</td>\n",
       "      <td>-0.071727</td>\n",
       "      <td>-0.066854</td>\n",
       "      <td>0.086834</td>\n",
       "      <td>-0.251208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222685</td>\n",
       "      <td>-0.630494</td>\n",
       "      <td>0.102872</td>\n",
       "      <td>-0.372469</td>\n",
       "      <td>0.117597</td>\n",
       "      <td>0.124286</td>\n",
       "      <td>-0.001315</td>\n",
       "      <td>0.021587</td>\n",
       "      <td>2.635848</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>499</td>\n",
       "      <td>-0.554782</td>\n",
       "      <td>0.392480</td>\n",
       "      <td>1.496188</td>\n",
       "      <td>0.269426</td>\n",
       "      <td>0.740235</td>\n",
       "      <td>-0.929436</td>\n",
       "      <td>0.744993</td>\n",
       "      <td>-0.165763</td>\n",
       "      <td>-0.115377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.093165</td>\n",
       "      <td>-0.192106</td>\n",
       "      <td>0.378167</td>\n",
       "      <td>0.362084</td>\n",
       "      <td>-0.360221</td>\n",
       "      <td>-0.090399</td>\n",
       "      <td>-0.099002</td>\n",
       "      <td>1.049528</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>119</td>\n",
       "      <td>1.219421</td>\n",
       "      <td>0.356318</td>\n",
       "      <td>0.306976</td>\n",
       "      <td>0.675745</td>\n",
       "      <td>-0.335587</td>\n",
       "      <td>-1.030548</td>\n",
       "      <td>0.093076</td>\n",
       "      <td>-0.192551</td>\n",
       "      <td>0.030401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.282820</td>\n",
       "      <td>-0.819140</td>\n",
       "      <td>0.128772</td>\n",
       "      <td>0.288253</td>\n",
       "      <td>0.167725</td>\n",
       "      <td>0.094124</td>\n",
       "      <td>-0.015690</td>\n",
       "      <td>0.036879</td>\n",
       "      <td>2.638928</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>230</td>\n",
       "      <td>-0.135090</td>\n",
       "      <td>0.519203</td>\n",
       "      <td>0.720383</td>\n",
       "      <td>0.129065</td>\n",
       "      <td>0.852819</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>0.417669</td>\n",
       "      <td>0.077835</td>\n",
       "      <td>-0.127021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088125</td>\n",
       "      <td>-0.243794</td>\n",
       "      <td>0.065839</td>\n",
       "      <td>-0.999654</td>\n",
       "      <td>-1.018342</td>\n",
       "      <td>-0.034811</td>\n",
       "      <td>0.161734</td>\n",
       "      <td>0.156045</td>\n",
       "      <td>0.992083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>288</td>\n",
       "      <td>-0.598820</td>\n",
       "      <td>0.073254</td>\n",
       "      <td>-0.113786</td>\n",
       "      <td>-2.315189</td>\n",
       "      <td>1.644382</td>\n",
       "      <td>4.389522</td>\n",
       "      <td>3.174203</td>\n",
       "      <td>-1.032390</td>\n",
       "      <td>0.673033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.588539</td>\n",
       "      <td>-0.496473</td>\n",
       "      <td>-0.276571</td>\n",
       "      <td>1.069602</td>\n",
       "      <td>-0.629078</td>\n",
       "      <td>0.401503</td>\n",
       "      <td>-1.346578</td>\n",
       "      <td>-1.228507</td>\n",
       "      <td>380.950000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>445</td>\n",
       "      <td>-2.755146</td>\n",
       "      <td>-1.142447</td>\n",
       "      <td>0.024379</td>\n",
       "      <td>2.962694</td>\n",
       "      <td>0.617638</td>\n",
       "      <td>-1.207468</td>\n",
       "      <td>-0.803439</td>\n",
       "      <td>0.507743</td>\n",
       "      <td>-1.256491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604726</td>\n",
       "      <td>0.249924</td>\n",
       "      <td>0.649895</td>\n",
       "      <td>-0.051671</td>\n",
       "      <td>0.187015</td>\n",
       "      <td>-0.017907</td>\n",
       "      <td>-0.050109</td>\n",
       "      <td>-0.034841</td>\n",
       "      <td>320.388014</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>500</td>\n",
       "      <td>-2.482916</td>\n",
       "      <td>-1.778236</td>\n",
       "      <td>1.130820</td>\n",
       "      <td>1.023312</td>\n",
       "      <td>1.171128</td>\n",
       "      <td>-0.732718</td>\n",
       "      <td>0.301305</td>\n",
       "      <td>0.021748</td>\n",
       "      <td>-0.144468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300289</td>\n",
       "      <td>0.165891</td>\n",
       "      <td>0.672330</td>\n",
       "      <td>-0.269605</td>\n",
       "      <td>0.015657</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>-0.292450</td>\n",
       "      <td>-0.144383</td>\n",
       "      <td>302.165426</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>476</td>\n",
       "      <td>-2.175784</td>\n",
       "      <td>-1.698843</td>\n",
       "      <td>1.356187</td>\n",
       "      <td>1.424776</td>\n",
       "      <td>1.198425</td>\n",
       "      <td>-0.997624</td>\n",
       "      <td>0.552596</td>\n",
       "      <td>-0.104218</td>\n",
       "      <td>-0.218105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417487</td>\n",
       "      <td>0.372266</td>\n",
       "      <td>0.705728</td>\n",
       "      <td>-0.015201</td>\n",
       "      <td>0.325461</td>\n",
       "      <td>-0.271905</td>\n",
       "      <td>-0.191930</td>\n",
       "      <td>-0.030495</td>\n",
       "      <td>312.414820</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>490</td>\n",
       "      <td>-2.674531</td>\n",
       "      <td>-2.249585</td>\n",
       "      <td>1.116343</td>\n",
       "      <td>1.455786</td>\n",
       "      <td>1.235615</td>\n",
       "      <td>-0.846227</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>-0.008856</td>\n",
       "      <td>-0.187699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423814</td>\n",
       "      <td>0.258032</td>\n",
       "      <td>0.912823</td>\n",
       "      <td>-0.277875</td>\n",
       "      <td>0.105937</td>\n",
       "      <td>-0.036300</td>\n",
       "      <td>-0.278889</td>\n",
       "      <td>-0.082811</td>\n",
       "      <td>379.694596</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>284</td>\n",
       "      <td>-1.167212</td>\n",
       "      <td>1.263648</td>\n",
       "      <td>-0.109849</td>\n",
       "      <td>-0.783619</td>\n",
       "      <td>-1.472694</td>\n",
       "      <td>-0.212799</td>\n",
       "      <td>1.732683</td>\n",
       "      <td>-0.882945</td>\n",
       "      <td>-0.331912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770748</td>\n",
       "      <td>0.105265</td>\n",
       "      <td>0.127680</td>\n",
       "      <td>0.509606</td>\n",
       "      <td>-0.401951</td>\n",
       "      <td>0.835236</td>\n",
       "      <td>0.286015</td>\n",
       "      <td>0.067264</td>\n",
       "      <td>302.100000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>487</td>\n",
       "      <td>-2.747415</td>\n",
       "      <td>-2.428872</td>\n",
       "      <td>1.110836</td>\n",
       "      <td>1.620286</td>\n",
       "      <td>1.260144</td>\n",
       "      <td>-0.889403</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>-0.020497</td>\n",
       "      <td>-0.204143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470798</td>\n",
       "      <td>0.293080</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>-0.281021</td>\n",
       "      <td>0.140277</td>\n",
       "      <td>-0.057841</td>\n",
       "      <td>-0.273731</td>\n",
       "      <td>-0.059391</td>\n",
       "      <td>409.184350</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>181</td>\n",
       "      <td>-0.723637</td>\n",
       "      <td>0.154496</td>\n",
       "      <td>1.263163</td>\n",
       "      <td>0.964978</td>\n",
       "      <td>-0.027170</td>\n",
       "      <td>2.285202</td>\n",
       "      <td>1.891703</td>\n",
       "      <td>-0.141478</td>\n",
       "      <td>-0.110655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078788</td>\n",
       "      <td>0.424114</td>\n",
       "      <td>0.206428</td>\n",
       "      <td>-0.966411</td>\n",
       "      <td>-0.761682</td>\n",
       "      <td>-0.437223</td>\n",
       "      <td>-0.051109</td>\n",
       "      <td>-0.380738</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>474</td>\n",
       "      <td>-2.584438</td>\n",
       "      <td>-2.385681</td>\n",
       "      <td>1.230107</td>\n",
       "      <td>1.831599</td>\n",
       "      <td>1.274424</td>\n",
       "      <td>-1.029270</td>\n",
       "      <td>0.445684</td>\n",
       "      <td>-0.087064</td>\n",
       "      <td>-0.242993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532493</td>\n",
       "      <td>0.402034</td>\n",
       "      <td>1.021365</td>\n",
       "      <td>-0.146404</td>\n",
       "      <td>0.303957</td>\n",
       "      <td>-0.212312</td>\n",
       "      <td>-0.220583</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>414.411734</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>450</td>\n",
       "      <td>-2.809714</td>\n",
       "      <td>-1.523688</td>\n",
       "      <td>0.225719</td>\n",
       "      <td>2.835153</td>\n",
       "      <td>0.758067</td>\n",
       "      <td>-1.180478</td>\n",
       "      <td>-0.589813</td>\n",
       "      <td>0.398843</td>\n",
       "      <td>-1.070013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615506</td>\n",
       "      <td>0.285034</td>\n",
       "      <td>0.787278</td>\n",
       "      <td>-0.097486</td>\n",
       "      <td>0.204571</td>\n",
       "      <td>-0.042023</td>\n",
       "      <td>-0.088456</td>\n",
       "      <td>-0.021481</td>\n",
       "      <td>359.860499</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.660000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>485</td>\n",
       "      <td>-2.805425</td>\n",
       "      <td>-3.006210</td>\n",
       "      <td>1.403224</td>\n",
       "      <td>2.026415</td>\n",
       "      <td>1.611444</td>\n",
       "      <td>-0.686045</td>\n",
       "      <td>-0.230521</td>\n",
       "      <td>0.127667</td>\n",
       "      <td>0.009808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607075</td>\n",
       "      <td>0.608970</td>\n",
       "      <td>1.129823</td>\n",
       "      <td>-0.422408</td>\n",
       "      <td>0.165846</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>-0.223516</td>\n",
       "      <td>-0.011455</td>\n",
       "      <td>408.569136</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>550</td>\n",
       "      <td>0.601636</td>\n",
       "      <td>-1.634335</td>\n",
       "      <td>1.042738</td>\n",
       "      <td>1.063768</td>\n",
       "      <td>-1.872933</td>\n",
       "      <td>0.247781</td>\n",
       "      <td>-0.699909</td>\n",
       "      <td>0.211613</td>\n",
       "      <td>-0.248833</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.302662</td>\n",
       "      <td>-0.926639</td>\n",
       "      <td>-0.144534</td>\n",
       "      <td>0.484127</td>\n",
       "      <td>0.113315</td>\n",
       "      <td>-0.498560</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.084220</td>\n",
       "      <td>317.250000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>488</td>\n",
       "      <td>-2.746307</td>\n",
       "      <td>-2.968696</td>\n",
       "      <td>1.481370</td>\n",
       "      <td>1.961311</td>\n",
       "      <td>1.673919</td>\n",
       "      <td>-0.592006</td>\n",
       "      <td>-0.368584</td>\n",
       "      <td>0.176195</td>\n",
       "      <td>0.079513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593515</td>\n",
       "      <td>0.652043</td>\n",
       "      <td>1.068713</td>\n",
       "      <td>-0.454337</td>\n",
       "      <td>0.137556</td>\n",
       "      <td>0.141443</td>\n",
       "      <td>-0.216252</td>\n",
       "      <td>-0.023178</td>\n",
       "      <td>378.669557</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>93</td>\n",
       "      <td>-0.853807</td>\n",
       "      <td>0.127392</td>\n",
       "      <td>1.267277</td>\n",
       "      <td>0.678584</td>\n",
       "      <td>-1.029851</td>\n",
       "      <td>-0.487614</td>\n",
       "      <td>1.836071</td>\n",
       "      <td>-0.298566</td>\n",
       "      <td>-0.922127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252358</td>\n",
       "      <td>0.179725</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.724705</td>\n",
       "      <td>0.526798</td>\n",
       "      <td>0.502701</td>\n",
       "      <td>-0.159465</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>322.440000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "190    125 -2.807065  1.183098  2.974779  2.801477 -0.475110  3.217829   \n",
       "608    460 -2.264656 -0.707192  1.080243 -4.657545 -0.157183 -0.345406   \n",
       "357    263  1.143649  0.128210  0.407255  1.160950 -0.504532 -0.904843   \n",
       "751    561 -0.711797 -0.983416  2.042490 -2.348505 -1.670333 -0.407697   \n",
       "158     99 -0.883996 -0.150765  2.291791 -0.263452 -0.814535  0.955841   \n",
       "169    108 -0.817164  0.548008  1.987429  2.347620  0.036888  1.509523   \n",
       "364    266  0.982539 -0.229085  0.003051  1.444009  0.200645  0.636756   \n",
       "1010   511 -2.320490 -2.698492  2.044250  1.492373  2.123919  0.085353   \n",
       "224    147  0.912979 -0.653000  0.298165  0.209546 -0.197231  0.971100   \n",
       "306    221 -0.342871 -0.199546  1.976353 -0.003495 -1.170366  0.883501   \n",
       "267    190 -0.913600  0.162262  0.541429 -1.931799  0.235402 -0.209263   \n",
       "207    138 -0.986640  0.054620  0.892189  1.238546  1.153949 -0.443157   \n",
       "645    489 -0.932349  0.789123  0.534111  1.140724 -0.448967 -0.288526   \n",
       "157     98  1.027114 -1.272543  0.673656 -0.747436 -1.299107  0.293656   \n",
       "804    494 -1.182051 -0.287028  1.913052  0.409380  1.320550 -0.546925   \n",
       "104     69 -1.766645  2.352984 -0.009955 -0.363736  1.460953 -0.204833   \n",
       "1182   344 -0.492952  0.464595  1.181024  0.153257  0.904070 -0.400310   \n",
       "959    263 -0.238061  0.503490  0.852928  0.136026  0.867566 -0.107017   \n",
       "386    282 -0.669415 -0.169044  1.301837  0.370525  0.950911 -0.413911   \n",
       "846    415 -2.288051  1.606998 -1.293595  3.776220 -0.290702 -1.269790   \n",
       "710    536 -0.736062  0.104864  2.356355 -1.796916 -0.727809 -0.177561   \n",
       "879    435 -1.788964  1.364592 -0.343040  2.555589  0.040555 -1.227869   \n",
       "285    204 -0.188424  0.877602 -0.734686 -0.913404  1.941770  4.037423   \n",
       "328    241 -1.142321  0.626405  2.526917  2.827973  0.619263  0.897473   \n",
       "698    528 -0.378417  0.751515  1.772256  0.311020 -0.329130 -0.746206   \n",
       "884      5  1.156233  0.275225  0.175572  0.437541  0.084320 -0.071727   \n",
       "1525   499 -0.554782  0.392480  1.496188  0.269426  0.740235 -0.929436   \n",
       "1312   119  1.219421  0.356318  0.306976  0.675745 -0.335587 -1.030548   \n",
       "1524   230 -0.135090  0.519203  0.720383  0.129065  0.852819  0.011468   \n",
       "396    288 -0.598820  0.073254 -0.113786 -2.315189  1.644382  4.389522   \n",
       "850    445 -2.755146 -1.142447  0.024379  2.962694  0.617638 -1.207468   \n",
       "1506   500 -2.482916 -1.778236  1.130820  1.023312  1.171128 -0.732718   \n",
       "1231   476 -2.175784 -1.698843  1.356187  1.424776  1.198425 -0.997624   \n",
       "1497   490 -2.674531 -2.249585  1.116343  1.455786  1.235615 -0.846227   \n",
       "392    284 -1.167212  1.263648 -0.109849 -0.783619 -1.472694 -0.212799   \n",
       "877    487 -2.747415 -2.428872  1.110836  1.620286  1.260144 -0.889403   \n",
       "257    181 -0.723637  0.154496  1.263163  0.964978 -0.027170  2.285202   \n",
       "1408   474 -2.584438 -2.385681  1.230107  1.831599  1.274424 -1.029270   \n",
       "1138   450 -2.809714 -1.523688  0.225719  2.835153  0.758067 -1.180478   \n",
       "2        1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "999    485 -2.805425 -3.006210  1.403224  2.026415  1.611444 -0.686045   \n",
       "732    550  0.601636 -1.634335  1.042738  1.063768 -1.872933  0.247781   \n",
       "784    488 -2.746307 -2.968696  1.481370  1.961311  1.673919 -0.592006   \n",
       "149     93 -0.853807  0.127392  1.267277  0.678584 -1.029851 -0.487614   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "190  -0.401608  0.384395  2.408675  ... -0.201782  1.461535 -0.000330   \n",
       "608   0.595042 -0.386704  2.211141  ... -0.632027  0.222079 -0.189412   \n",
       "357   0.111993 -0.151667  0.180408  ... -0.036001 -0.221754 -0.049750   \n",
       "751  -0.243285  0.098952 -2.324725  ...  0.230367  0.659188  0.153339   \n",
       "158   0.097631  0.474047  0.139512  ...  0.070901  0.051832  0.110298   \n",
       "169   0.692961  0.508962 -1.254297  ... -0.047336 -0.324229  0.231980   \n",
       "364   0.012166  0.128519 -0.361986  ... -0.315217 -1.347024 -0.033817   \n",
       "1010 -1.363037  0.525734  0.581591  ...  0.495838  0.962297  0.628542   \n",
       "224  -0.449461  0.281896  0.262493  ...  0.089258 -0.077306 -0.237707   \n",
       "306  -0.151879  0.160106  0.137973  ... -0.313443  0.086207  0.109600   \n",
       "267   0.770523 -0.407195 -1.374754  ... -0.382552 -0.546739 -0.320022   \n",
       "207   1.158793 -0.234933 -0.748284  ... -0.156084 -0.590576  0.432953   \n",
       "645   1.072543  0.118515 -0.152883  ...  0.092532  0.662627  0.319256   \n",
       "157  -1.054200  0.234900 -0.574991  ...  0.522113  1.101912 -0.248747   \n",
       "804   0.171175  0.067171  0.118582  ...  0.150677  0.497816 -0.126352   \n",
       "104   0.905819 -3.384123  0.388546  ...  1.964253 -0.883218 -0.247698   \n",
       "1182  0.625865 -0.027956 -0.133834  ... -0.018412 -0.006796 -0.080287   \n",
       "959   0.477575  0.047395 -0.128981  ... -0.068066 -0.175600  0.023793   \n",
       "386  -0.149038  0.127021  0.463116  ...  0.297490  0.890770  0.012742   \n",
       "846  -2.504249  1.344858 -2.480836  ...  0.509880  0.060397 -0.406021   \n",
       "710  -0.264346  0.179152 -0.672160  ...  0.132683  0.432163 -0.347870   \n",
       "879  -1.245844  0.806355 -1.776606  ...  0.346781  0.084573 -0.386864   \n",
       "285  -1.707118 -2.537641 -0.590338  ... -0.824079 -1.042331  0.168265   \n",
       "328   0.536278 -0.060163 -0.813749  ... -0.309746 -0.269173  0.177396   \n",
       "698   0.719034 -0.081805 -0.152417  ... -0.120891 -0.240394 -0.057803   \n",
       "884  -0.066854  0.086834 -0.251208  ... -0.222685 -0.630494  0.102872   \n",
       "1525  0.744993 -0.165763 -0.115377  ...  0.006030  0.093165 -0.192106   \n",
       "1312  0.093076 -0.192551  0.030401  ... -0.282820 -0.819140  0.128772   \n",
       "1524  0.417669  0.077835 -0.127021  ... -0.088125 -0.243794  0.065839   \n",
       "396   3.174203 -1.032390  0.673033  ... -0.588539 -0.496473 -0.276571   \n",
       "850  -0.803439  0.507743 -1.256491  ...  0.604726  0.249924  0.649895   \n",
       "1506  0.301305  0.021748 -0.144468  ...  0.300289  0.165891  0.672330   \n",
       "1231  0.552596 -0.104218 -0.218105  ...  0.417487  0.372266  0.705728   \n",
       "1497  0.309600 -0.008856 -0.187699  ...  0.423814  0.258032  0.912823   \n",
       "392   1.732683 -0.882945 -0.331912  ...  0.770748  0.105265  0.127680   \n",
       "877   0.312755 -0.020497 -0.204143  ...  0.470798  0.293080  1.004300   \n",
       "257   1.891703 -0.141478 -0.110655  ... -0.078788  0.424114  0.206428   \n",
       "1408  0.445684 -0.087064 -0.242993  ...  0.532493  0.402034  1.021365   \n",
       "1138 -0.589813  0.398843 -1.070013  ...  0.615506  0.285034  0.787278   \n",
       "2     0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n",
       "999  -0.230521  0.127667  0.009808  ...  0.607075  0.608970  1.129823   \n",
       "732  -0.699909  0.211613 -0.248833  ... -0.302662 -0.926639 -0.144534   \n",
       "784  -0.368584  0.176195  0.079513  ...  0.593515  0.652043  1.068713   \n",
       "149   1.836071 -0.298566 -0.922127  ...  0.252358  0.179725  0.476744   \n",
       "\n",
       "           V24       V25       V26       V27       V28      Amount  clusters  \n",
       "190  -0.581133 -0.101785  0.529386  0.362411  0.237647   43.710000         8  \n",
       "608  -0.332603  0.050197 -1.168870 -0.208793 -0.486754   50.780000         8  \n",
       "357   0.345483  0.495658 -0.440443  0.008209  0.031689   49.990000         8  \n",
       "751   0.486741 -0.021313 -0.232520  0.060912  0.115507  149.900000         5  \n",
       "158  -0.260629 -0.097549  1.155439 -0.021199  0.062565  142.710000         5  \n",
       "169  -0.335248  0.161932  0.014694 -0.028691 -0.015257  148.430000         5  \n",
       "364  -1.359832 -0.005459  0.706075 -0.091822  0.024234  154.400000         5  \n",
       "1010 -0.684317 -0.066222  0.552321 -0.163931 -0.107619  163.305657         5  \n",
       "224  -1.301415  0.179599  0.469956 -0.024739  0.028338  170.430000         5  \n",
       "306  -0.098951 -0.943009 -0.618657  0.253306  0.240271   99.820000         5  \n",
       "267  -0.928385 -0.080009  0.908687 -0.286881  0.140450   99.950000         5  \n",
       "207  -0.337631 -0.272391 -0.551475  0.171670  0.012712  148.810000         5  \n",
       "645   0.397955 -0.454037 -0.338746  0.134917 -0.094342  139.750000         5  \n",
       "157  -0.259414  0.296058 -0.030996  0.018465  0.033482  159.000000         5  \n",
       "804   0.091163  0.246565 -0.142492 -0.109225 -0.136491    1.118400         0  \n",
       "104  -0.758606  0.086450  0.202790 -0.898858 -0.944337    0.760000         0  \n",
       "1182 -0.374617 -0.382285 -0.223915  0.041612  0.028870    0.995656         0  \n",
       "959  -0.819806 -0.835323 -0.089224  0.127170  0.119452    0.993111         0  \n",
       "386  -0.407764 -0.678186 -0.513178  0.253350  0.254336   11.640000         0  \n",
       "846   0.228874  0.024053  0.230834  0.231221 -0.145431    0.116356         0  \n",
       "710  -0.126574  0.487953 -0.235765  0.304176  0.126340   15.000000         0  \n",
       "879   0.344841  0.175548 -0.060967  0.122936 -0.136656    0.378042         0  \n",
       "285   0.963855  0.887850  0.171167  0.012284  0.164538    1.980000         0  \n",
       "328  -0.019578  0.048651  0.068831 -0.246503 -0.230837   10.620000         0  \n",
       "698   0.733812 -0.049448  0.207357  0.023386  0.057469   25.410000         0  \n",
       "884  -0.372469  0.117597  0.124286 -0.001315  0.021587    2.635848         0  \n",
       "1525  0.378167  0.362084 -0.360221 -0.090399 -0.099002    1.049528         0  \n",
       "1312  0.288253  0.167725  0.094124 -0.015690  0.036879    2.638928         0  \n",
       "1524 -0.999654 -1.018342 -0.034811  0.161734  0.156045    0.992083         0  \n",
       "396   1.069602 -0.629078  0.401503 -1.346578 -1.228507  380.950000         7  \n",
       "850  -0.051671  0.187015 -0.017907 -0.050109 -0.034841  320.388014         7  \n",
       "1506 -0.269605  0.015657  0.020332 -0.292450 -0.144383  302.165426         7  \n",
       "1231 -0.015201  0.325461 -0.271905 -0.191930 -0.030495  312.414820         7  \n",
       "1497 -0.277875  0.105937 -0.036300 -0.278889 -0.082811  379.694596         7  \n",
       "392   0.509606 -0.401951  0.835236  0.286015  0.067264  302.100000         7  \n",
       "877  -0.281021  0.140277 -0.057841 -0.273731 -0.059391  409.184350         7  \n",
       "257  -0.966411 -0.761682 -0.437223 -0.051109 -0.380738  318.500000         7  \n",
       "1408 -0.146404  0.303957 -0.212312 -0.220583  0.000709  414.411734         7  \n",
       "1138 -0.097486  0.204571 -0.042023 -0.088456 -0.021481  359.860499         7  \n",
       "2    -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.660000         7  \n",
       "999  -0.422408  0.165846  0.084400 -0.223516 -0.011455  408.569136         7  \n",
       "732   0.484127  0.113315 -0.498560  0.028261  0.084220  317.250000         7  \n",
       "784  -0.454337  0.137556  0.141443 -0.216252 -0.023178  378.669557         7  \n",
       "149   0.724705  0.526798  0.502701 -0.159465  0.002761  322.440000         7  \n",
       "\n",
       "[44 rows x 31 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25\n",
       "1    19\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 4}\n",
      "KNeighborsClassifier accuracy: 53.31%\n",
      "{'alpha': 1}\n",
      "BernoulliNB accuracy: 75.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "LogisticRegression accuracy: 81.92%\n",
      "{'max_depth': 5}\n",
      "DecisionTreeClassifier accuracy: 81.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'n_estimators': 100}\n",
      "RandomForestClassifier accuracy: 89.27%\n",
      "{'C': 1, 'gamma': 0.001}\n",
      "SVC accuracy: 61.54%\n"
     ]
    }
   ],
   "source": [
    "clus_accuracy=[]\n",
    "for model, params in models:\n",
    "    grid = GridSearchCV(model, params, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    print(best_params)\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    accuracy = best_model.score(X_test, y_test)\n",
    "    clus_accuracy.append(accuracy)\n",
    "    print(type(model).__name__ + ' accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>SRS</th>\n",
       "      <th>SS</th>\n",
       "      <th>Stra</th>\n",
       "      <th>clus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kneighbors</td>\n",
       "      <td>0.803870</td>\n",
       "      <td>0.637849</td>\n",
       "      <td>0.796902</td>\n",
       "      <td>0.533063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.848725</td>\n",
       "      <td>0.773315</td>\n",
       "      <td>0.858864</td>\n",
       "      <td>0.758435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.913808</td>\n",
       "      <td>0.843431</td>\n",
       "      <td>0.902754</td>\n",
       "      <td>0.819163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision</td>\n",
       "      <td>0.940193</td>\n",
       "      <td>0.862491</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.816464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.915589</td>\n",
       "      <td>0.989673</td>\n",
       "      <td>0.892713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.871592</td>\n",
       "      <td>0.677332</td>\n",
       "      <td>0.889845</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Models       SRS        SS      Stra      clus\n",
       "0  Kneighbors  0.803870  0.637849  0.796902  0.533063\n",
       "1   Bernoulli  0.848725  0.773315  0.858864  0.758435\n",
       "2    logistic  0.913808  0.843431  0.902754  0.819163\n",
       "3    Decision  0.940193  0.862491  0.945783  0.816464\n",
       "4      Random  0.992084  0.915589  0.989673  0.892713\n",
       "5         SVC  0.871592  0.677332  0.889845  0.615385"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['clus'] = clus_accuracy\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new,y_new,test_size=0.3,random_state=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 2}\n",
      "KNeighborsClassifier accuracy: 84.06%\n",
      "{'alpha': 0.5}\n",
      "BernoulliNB accuracy: 84.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100}\n",
      "LogisticRegression accuracy: 92.14%\n",
      "{'max_depth': 15}\n",
      "DecisionTreeClassifier accuracy: 98.25%\n",
      "{'max_depth': 20, 'n_estimators': 100}\n",
      "RandomForestClassifier accuracy: 99.13%\n",
      "{'C': 100, 'gamma': 0.001}\n",
      "SVC accuracy: 92.58%\n"
     ]
    }
   ],
   "source": [
    "normal_accuracy=[]\n",
    "for model, params in models:\n",
    "    grid = GridSearchCV(model, params, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    print(best_params)\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    accuracy = best_model.score(X_test, y_test)\n",
    "    normal_accuracy.append(accuracy)\n",
    "    print(type(model).__name__ + ' accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>SRS</th>\n",
       "      <th>SS</th>\n",
       "      <th>Stra</th>\n",
       "      <th>clus</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kneighbors</td>\n",
       "      <td>0.803870</td>\n",
       "      <td>0.637849</td>\n",
       "      <td>0.796902</td>\n",
       "      <td>0.533063</td>\n",
       "      <td>0.840611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.848725</td>\n",
       "      <td>0.773315</td>\n",
       "      <td>0.858864</td>\n",
       "      <td>0.758435</td>\n",
       "      <td>0.849345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.913808</td>\n",
       "      <td>0.843431</td>\n",
       "      <td>0.902754</td>\n",
       "      <td>0.819163</td>\n",
       "      <td>0.921397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision</td>\n",
       "      <td>0.940193</td>\n",
       "      <td>0.862491</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.816464</td>\n",
       "      <td>0.982533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.915589</td>\n",
       "      <td>0.989673</td>\n",
       "      <td>0.892713</td>\n",
       "      <td>0.991266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.871592</td>\n",
       "      <td>0.677332</td>\n",
       "      <td>0.889845</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.925764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Models       SRS        SS      Stra      clus    normal\n",
       "0  Kneighbors  0.803870  0.637849  0.796902  0.533063  0.840611\n",
       "1   Bernoulli  0.848725  0.773315  0.858864  0.758435  0.849345\n",
       "2    logistic  0.913808  0.843431  0.902754  0.819163  0.921397\n",
       "3    Decision  0.940193  0.862491  0.945783  0.816464  0.982533\n",
       "4      Random  0.992084  0.915589  0.989673  0.892713  0.991266\n",
       "5         SVC  0.871592  0.677332  0.889845  0.615385  0.925764"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['normal'] = normal_accuracy\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SRS</th>\n",
       "      <th>SS</th>\n",
       "      <th>Stra</th>\n",
       "      <th>clus</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kneighbors</th>\n",
       "      <td>0.803870</td>\n",
       "      <td>0.637849</td>\n",
       "      <td>0.796902</td>\n",
       "      <td>0.533063</td>\n",
       "      <td>0.840611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli</th>\n",
       "      <td>0.848725</td>\n",
       "      <td>0.773315</td>\n",
       "      <td>0.858864</td>\n",
       "      <td>0.758435</td>\n",
       "      <td>0.849345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic</th>\n",
       "      <td>0.913808</td>\n",
       "      <td>0.843431</td>\n",
       "      <td>0.902754</td>\n",
       "      <td>0.819163</td>\n",
       "      <td>0.921397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision</th>\n",
       "      <td>0.940193</td>\n",
       "      <td>0.862491</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.816464</td>\n",
       "      <td>0.982533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random</th>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.915589</td>\n",
       "      <td>0.989673</td>\n",
       "      <td>0.892713</td>\n",
       "      <td>0.991266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.871592</td>\n",
       "      <td>0.677332</td>\n",
       "      <td>0.889845</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.925764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 SRS        SS      Stra      clus    normal\n",
       "Models                                                      \n",
       "Kneighbors  0.803870  0.637849  0.796902  0.533063  0.840611\n",
       "Bernoulli   0.848725  0.773315  0.858864  0.758435  0.849345\n",
       "logistic    0.913808  0.843431  0.902754  0.819163  0.921397\n",
       "Decision    0.940193  0.862491  0.945783  0.816464  0.982533\n",
       "Random      0.992084  0.915589  0.989673  0.892713  0.991266\n",
       "SVC         0.871592  0.677332  0.889845  0.615385  0.925764"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top= table\n",
    "table = table.set_index('Models')\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAXIMUM ACCURACY SAMPLE FOR ALL MODELS\n",
    "\n",
    "\n",
    "The random forest model gives best accuracy for all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SRS       0.992084\n",
       "SS        0.915589\n",
       "Stra      0.989673\n",
       "clus      0.892713\n",
       "normal    0.991266\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_values = table.max()\n",
    "max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SRS       Random\n",
       "SS        Random\n",
       "Stra      Random\n",
       "clus      Random\n",
       "normal    Random\n",
       "dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index = table.idxmax()\n",
    "max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "best= pd.DataFrame([max_index,max_values])\n",
    "best.index = ['Model','Accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SRS</th>\n",
       "      <th>SS</th>\n",
       "      <th>Stra</th>\n",
       "      <th>clus</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>Random</td>\n",
       "      <td>Random</td>\n",
       "      <td>Random</td>\n",
       "      <td>Random</td>\n",
       "      <td>Random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.915589</td>\n",
       "      <td>0.989673</td>\n",
       "      <td>0.892713</td>\n",
       "      <td>0.991266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SRS        SS      Stra      clus    normal\n",
       "Model       Random    Random    Random    Random    Random\n",
       "Accuracy  0.992084  0.915589  0.989673  0.892713  0.991266"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ec044e8ff643357d7a8bcf5df711256ba0d7dda7c0007fb25b823abc5f121fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
